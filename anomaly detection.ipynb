{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers,Model\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r\"C:\\Users\\bzhang\\Desktop\\processminer-rare-event-mts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>...</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5-1-99 0:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376665</td>\n",
       "      <td>-4.596435</td>\n",
       "      <td>-4.095756</td>\n",
       "      <td>13.497687</td>\n",
       "      <td>-0.118830</td>\n",
       "      <td>-20.669883</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.091721</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>-4.936434</td>\n",
       "      <td>-24.590146</td>\n",
       "      <td>18.515436</td>\n",
       "      <td>3.473400</td>\n",
       "      <td>0.033444</td>\n",
       "      <td>0.953219</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5-1-99 0:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475720</td>\n",
       "      <td>-4.542502</td>\n",
       "      <td>-4.018359</td>\n",
       "      <td>16.230659</td>\n",
       "      <td>-0.128733</td>\n",
       "      <td>-18.758079</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.095871</td>\n",
       "      <td>0.062801</td>\n",
       "      <td>-4.937179</td>\n",
       "      <td>-32.413266</td>\n",
       "      <td>22.760065</td>\n",
       "      <td>2.682933</td>\n",
       "      <td>0.033536</td>\n",
       "      <td>1.090502</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5-1-99 0:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363848</td>\n",
       "      <td>-4.681394</td>\n",
       "      <td>-4.353147</td>\n",
       "      <td>14.127998</td>\n",
       "      <td>-0.138636</td>\n",
       "      <td>-17.836632</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.100265</td>\n",
       "      <td>0.072322</td>\n",
       "      <td>-4.937924</td>\n",
       "      <td>-34.183774</td>\n",
       "      <td>27.004663</td>\n",
       "      <td>3.537487</td>\n",
       "      <td>0.033629</td>\n",
       "      <td>1.840540</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5-1-99 0:06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301590</td>\n",
       "      <td>-4.758934</td>\n",
       "      <td>-4.023612</td>\n",
       "      <td>13.161567</td>\n",
       "      <td>-0.148142</td>\n",
       "      <td>-18.517601</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.104660</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>-4.938669</td>\n",
       "      <td>-35.954281</td>\n",
       "      <td>21.672449</td>\n",
       "      <td>3.986095</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>2.554880</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5-1-99 0:08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265578</td>\n",
       "      <td>-4.749928</td>\n",
       "      <td>-4.333150</td>\n",
       "      <td>15.267340</td>\n",
       "      <td>-0.155314</td>\n",
       "      <td>-17.505913</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.109054</td>\n",
       "      <td>0.091121</td>\n",
       "      <td>-4.939414</td>\n",
       "      <td>-37.724789</td>\n",
       "      <td>21.907251</td>\n",
       "      <td>3.601573</td>\n",
       "      <td>0.033777</td>\n",
       "      <td>1.410494</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time  y        x1        x2        x3         x4        x5  \\\n",
       "0  5-1-99 0:00  0  0.376665 -4.596435 -4.095756  13.497687 -0.118830   \n",
       "1  5-1-99 0:02  0  0.475720 -4.542502 -4.018359  16.230659 -0.128733   \n",
       "2  5-1-99 0:04  0  0.363848 -4.681394 -4.353147  14.127998 -0.138636   \n",
       "3  5-1-99 0:06  0  0.301590 -4.758934 -4.023612  13.161567 -0.148142   \n",
       "4  5-1-99 0:08  0  0.265578 -4.749928 -4.333150  15.267340 -0.155314   \n",
       "\n",
       "          x6        x7        x8  ...        x52       x53       x54  \\\n",
       "0 -20.669883  0.000732 -0.061114  ...  10.091721  0.053279 -4.936434   \n",
       "1 -18.758079  0.000732 -0.061114  ...  10.095871  0.062801 -4.937179   \n",
       "2 -17.836632  0.010803 -0.061114  ...  10.100265  0.072322 -4.937924   \n",
       "3 -18.517601  0.002075 -0.061114  ...  10.104660  0.081600 -4.938669   \n",
       "4 -17.505913  0.000732 -0.061114  ...  10.109054  0.091121 -4.939414   \n",
       "\n",
       "         x55        x56       x57       x58       x59       x60  x61  \n",
       "0 -24.590146  18.515436  3.473400  0.033444  0.953219  0.006076    0  \n",
       "1 -32.413266  22.760065  2.682933  0.033536  1.090502  0.006083    0  \n",
       "2 -34.183774  27.004663  3.537487  0.033629  1.840540  0.006090    0  \n",
       "3 -35.954281  21.672449  3.986095  0.033721  2.554880  0.006097    0  \n",
       "4 -37.724789  21.907251  3.601573  0.033777  1.410494  0.006105    0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivatives = data.drop(columns = ['time','x61','x28','y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,len(first_derivatives)-1):\n",
    "    first_derivatives.iloc[i,:] = (first_derivatives.iloc[i+1,:]-first_derivatives.iloc[i-1,:])/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second_derivatives = data.drop(columns = ['time','x61','x28','y'])\n",
    "#for i in range(1,len(second_derivatives)-1):\n",
    "    #second_derivatives.iloc[i,:] = (second_derivatives.iloc[i+1,:]-2*second_derivatives.iloc[i,:]+second_derivatives.iloc[i-1,:])/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=first_derivatives.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_difference_cols = dict(zip(column_names,[('d'+name) for name in column_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_derivatives=first_derivatives.rename(columns = first_difference_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second_difference_cols = dict(zip(column_names,[('dd'+name) for name in column_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second_derivatives=second_derivatives.rename(columns = second_difference_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = ['time'],inplace = True)\n",
    "data_new_features = pd.concat([data,first_derivatives],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_new_features['dx28']=data_new_features['x28'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_features.dropna(axis = 0,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for i in data_new_features.loc[data_new_features.y==1].index:\n",
    "    data_new_features['y'][i]=0\n",
    "    data_new_features['y'][i-2:i]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18397, 122)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_new_features['y']\n",
    "x = data_new_features.drop(columns=['y'])\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.1,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = x_train.loc[y_train==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = x_test.loc[y_test==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=scaler.transform(training_data)\n",
    "test_data=scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data=np.array(training_data,'float32')\n",
    "test_data=np.array(test_data,'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_tensor(data):\n",
    "    sequence = []\n",
    "    for i in range(len(data)-5):\n",
    "        sequence.append(data[i:i+5])\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = prepare_tensor(training_data)\n",
    "test_data = prepare_tensor(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = tf.data.Dataset.from_tensor_slices(training_data).batch(64,drop_remainder=True)\n",
    "test_data=tf.data.Dataset.from_tensor_slices(test_data).batch(64,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,units,seq_len):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.lstm = layers.LSTM(units,activation = 'elu',recurrent_dropout=0.2,dropout = 0.2,return_sequences=True)\n",
    "        self.bottle_neck = layers.LSTM(units//2, activation='elu',return_sequences = False,recurrent_dropout=0.2,dropout = 0.2)\n",
    "        self.repeat = layers.RepeatVector(seq_len)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.lstm(inputs)\n",
    "        x = self.bottle_neck(x)\n",
    "        x = self.repeat(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self,units,n_features):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.lstm1 = layers.LSTM(units//2,activation = 'elu',recurrent_dropout=0.2,dropout = 0.2,return_sequences=True)\n",
    "        self.lstm2 = layers.LSTM(units, activation='elu',return_sequences = True,recurrent_dropout=0.2,dropout = 0.2)\n",
    "        self.decoder_output = layers.TimeDistributed(layers.Dense(n_features))\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.lstm1(inputs)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.decoder_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(Model):\n",
    "    def __init__(self,units,seq_len,n_features):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.encoder = Encoder(units,seq_len)\n",
    "        self.decoder = Decoder(units,n_features)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x = self.encoder(inputs)\n",
    "        y = self.decoder(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = AutoEncoder(64,5,121)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "test_loss = tf.keras.metrics.Mean('test_loss', dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, train_inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(train_inputs)\n",
    "        loss = tf.reduce_mean((train_inputs-predictions)**2)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    \n",
    "def test_step(model, test_inputs):\n",
    "    predictions = model(test_inputs)\n",
    "    loss = tf.reduce_mean((test_inputs-predictions)**2)\n",
    "\n",
    "    test_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9462159872055054, Test Loss: 2.424018144607544\n",
      "Epoch 2, Loss: 0.820109486579895, Test Loss: 2.2719712257385254\n",
      "Epoch 3, Loss: 0.7617450952529907, Test Loss: 2.1903746128082275\n",
      "Epoch 4, Loss: 0.7239757776260376, Test Loss: 2.1334216594696045\n",
      "Epoch 5, Loss: 0.6866888403892517, Test Loss: 2.034834146499634\n",
      "Epoch 6, Loss: 0.6529748439788818, Test Loss: 1.924087405204773\n",
      "Epoch 7, Loss: 0.6238954663276672, Test Loss: 1.8008259534835815\n",
      "Epoch 8, Loss: 0.5976119041442871, Test Loss: 1.7901045083999634\n",
      "Epoch 9, Loss: 0.5719148516654968, Test Loss: 1.8715628385543823\n",
      "Epoch 10, Loss: 0.5530669689178467, Test Loss: 1.5768619775772095\n",
      "Epoch 11, Loss: 0.535690426826477, Test Loss: 1.4546785354614258\n",
      "Epoch 12, Loss: 0.5183359980583191, Test Loss: 1.4626754522323608\n",
      "Epoch 13, Loss: 0.5043005347251892, Test Loss: 1.314872145652771\n",
      "Epoch 14, Loss: 0.48444414138793945, Test Loss: 1.234938621520996\n",
      "Epoch 15, Loss: 0.47159141302108765, Test Loss: 1.1686197519302368\n",
      "Epoch 16, Loss: 0.45999300479888916, Test Loss: 1.167499303817749\n",
      "Epoch 17, Loss: 0.45045381784439087, Test Loss: 1.1061567068099976\n",
      "Epoch 18, Loss: 0.4401453733444214, Test Loss: 1.0564547777175903\n",
      "Epoch 19, Loss: 0.4297201633453369, Test Loss: 1.0249947309494019\n",
      "Epoch 20, Loss: 0.4201919138431549, Test Loss: 0.9853278398513794\n",
      "Epoch 21, Loss: 0.40831953287124634, Test Loss: 0.996012270450592\n",
      "Epoch 22, Loss: 0.402896910905838, Test Loss: 0.9522250294685364\n",
      "Epoch 23, Loss: 0.3937035799026489, Test Loss: 0.9148716926574707\n",
      "Epoch 24, Loss: 0.38544347882270813, Test Loss: 0.885966420173645\n",
      "Epoch 25, Loss: 0.37970060110092163, Test Loss: 0.9214341044425964\n",
      "Epoch 26, Loss: 0.38186290860176086, Test Loss: 0.8994545340538025\n",
      "Epoch 27, Loss: 0.3693597912788391, Test Loss: 0.8343477249145508\n",
      "Epoch 28, Loss: 0.36914584040641785, Test Loss: 0.8572800755500793\n",
      "Epoch 29, Loss: 0.3606824278831482, Test Loss: 0.8203438520431519\n",
      "Epoch 30, Loss: 0.35189327597618103, Test Loss: 0.8216190934181213\n",
      "Epoch 31, Loss: 0.34483951330184937, Test Loss: 0.8186511993408203\n",
      "Epoch 32, Loss: 0.3407943844795227, Test Loss: 0.8006338477134705\n",
      "Epoch 33, Loss: 0.3300643265247345, Test Loss: 0.7707589268684387\n",
      "Epoch 34, Loss: 0.32651999592781067, Test Loss: 0.7885424494743347\n",
      "Epoch 35, Loss: 0.323501855134964, Test Loss: 0.789437472820282\n",
      "Epoch 36, Loss: 0.31870847940444946, Test Loss: 0.7631725668907166\n",
      "Epoch 37, Loss: 0.3144463300704956, Test Loss: 0.7514416575431824\n",
      "Epoch 38, Loss: 0.3124029040336609, Test Loss: 0.7853579521179199\n",
      "Epoch 39, Loss: 0.31223392486572266, Test Loss: 0.7798901200294495\n",
      "Epoch 40, Loss: 0.3063846528530121, Test Loss: 0.7256509065628052\n",
      "Epoch 41, Loss: 0.30493414402008057, Test Loss: 0.7715377807617188\n",
      "Epoch 42, Loss: 0.2976028621196747, Test Loss: 0.7265071868896484\n",
      "Epoch 43, Loss: 0.29265594482421875, Test Loss: 0.7177537083625793\n",
      "Epoch 44, Loss: 0.288499653339386, Test Loss: 0.7056659460067749\n",
      "Epoch 45, Loss: 0.2861476242542267, Test Loss: 0.7022603154182434\n",
      "Epoch 46, Loss: 0.2830888330936432, Test Loss: 0.726857602596283\n",
      "Epoch 47, Loss: 0.28040334582328796, Test Loss: 0.684927761554718\n",
      "Epoch 48, Loss: 0.277910977602005, Test Loss: 0.70478755235672\n",
      "Epoch 49, Loss: 0.27755868434906006, Test Loss: 0.6906794309616089\n",
      "Epoch 50, Loss: 0.2771812081336975, Test Loss: 0.7010183334350586\n",
      "Epoch 51, Loss: 0.27683916687965393, Test Loss: 0.6862717270851135\n",
      "Epoch 52, Loss: 0.2754638195037842, Test Loss: 0.6980276703834534\n",
      "Epoch 53, Loss: 0.27066493034362793, Test Loss: 0.6775075793266296\n",
      "Epoch 54, Loss: 0.2686184048652649, Test Loss: 0.675848662853241\n",
      "Epoch 55, Loss: 0.2655448317527771, Test Loss: 0.673073410987854\n",
      "Epoch 56, Loss: 0.26430392265319824, Test Loss: 0.6764228940010071\n",
      "Epoch 57, Loss: 0.2623184621334076, Test Loss: 0.6627106666564941\n",
      "Epoch 58, Loss: 0.2623271048069, Test Loss: 0.6752368211746216\n",
      "Epoch 59, Loss: 0.26046299934387207, Test Loss: 0.6402003169059753\n",
      "Epoch 60, Loss: 0.26389697194099426, Test Loss: 0.7119671106338501\n",
      "Epoch 61, Loss: 0.2638830542564392, Test Loss: 0.6725953221321106\n",
      "Epoch 62, Loss: 0.2557142972946167, Test Loss: 0.6542588472366333\n",
      "Epoch 63, Loss: 0.25096145272254944, Test Loss: 0.6371966004371643\n",
      "Epoch 64, Loss: 0.24845728278160095, Test Loss: 0.6484001278877258\n",
      "Epoch 65, Loss: 0.24739405512809753, Test Loss: 0.645217776298523\n",
      "Epoch 66, Loss: 0.24631546437740326, Test Loss: 0.6549686789512634\n",
      "Epoch 67, Loss: 0.2443532794713974, Test Loss: 0.6422786712646484\n",
      "Epoch 68, Loss: 0.2440478801727295, Test Loss: 0.6470047831535339\n",
      "Epoch 69, Loss: 0.24329009652137756, Test Loss: 0.6382616758346558\n",
      "Epoch 70, Loss: 0.2422381341457367, Test Loss: 0.642304003238678\n",
      "Epoch 71, Loss: 0.23922060430049896, Test Loss: 0.62160724401474\n",
      "Epoch 72, Loss: 0.23914995789527893, Test Loss: 0.6337109208106995\n",
      "Epoch 73, Loss: 0.2399480789899826, Test Loss: 0.635283887386322\n",
      "Epoch 74, Loss: 0.23623456060886383, Test Loss: 0.6317482590675354\n",
      "Epoch 75, Loss: 0.23651541769504547, Test Loss: 0.6085332036018372\n",
      "Epoch 76, Loss: 0.23527416586875916, Test Loss: 0.6494497656822205\n",
      "Epoch 77, Loss: 0.23430676758289337, Test Loss: 0.6125303506851196\n",
      "Epoch 78, Loss: 0.23232702910900116, Test Loss: 0.6363072395324707\n",
      "Epoch 79, Loss: 0.2334958016872406, Test Loss: 0.6266924738883972\n",
      "Epoch 80, Loss: 0.23284617066383362, Test Loss: 0.6244170069694519\n",
      "Epoch 81, Loss: 0.23075811564922333, Test Loss: 0.6048812866210938\n",
      "Epoch 82, Loss: 0.22825826704502106, Test Loss: 0.6291915774345398\n",
      "Epoch 83, Loss: 0.22593043744564056, Test Loss: 0.615998387336731\n",
      "Epoch 84, Loss: 0.2250881791114807, Test Loss: 0.6089650988578796\n",
      "Epoch 85, Loss: 0.22552376985549927, Test Loss: 0.6282532811164856\n",
      "Epoch 86, Loss: 0.22651924192905426, Test Loss: 0.6101325750350952\n",
      "Epoch 87, Loss: 0.22413621842861176, Test Loss: 0.6377162337303162\n",
      "Epoch 88, Loss: 0.2243977189064026, Test Loss: 0.6076592206954956\n",
      "Epoch 89, Loss: 0.22388236224651337, Test Loss: 0.6383888125419617\n",
      "Epoch 90, Loss: 0.22358819842338562, Test Loss: 0.6058562397956848\n",
      "Epoch 91, Loss: 0.22109685838222504, Test Loss: 0.6152853965759277\n",
      "Epoch 92, Loss: 0.22033855319023132, Test Loss: 0.615625262260437\n",
      "Epoch 93, Loss: 0.21669842302799225, Test Loss: 0.6047515273094177\n",
      "Epoch 94, Loss: 0.21439996361732483, Test Loss: 0.5876702666282654\n",
      "Epoch 95, Loss: 0.21377025544643402, Test Loss: 0.631338894367218\n",
      "Epoch 96, Loss: 0.21429382264614105, Test Loss: 0.6026806831359863\n",
      "Epoch 97, Loss: 0.21595686674118042, Test Loss: 0.6103619933128357\n",
      "Epoch 98, Loss: 0.2162134051322937, Test Loss: 0.6035882234573364\n",
      "Epoch 99, Loss: 0.21154522895812988, Test Loss: 0.5958348512649536\n",
      "Epoch 100, Loss: 0.2097422331571579, Test Loss: 0.5961247086524963\n",
      "Epoch 101, Loss: 0.20719081163406372, Test Loss: 0.6109792590141296\n",
      "Epoch 102, Loss: 0.20818087458610535, Test Loss: 0.6038900017738342\n",
      "Epoch 103, Loss: 0.20787614583969116, Test Loss: 0.5916576981544495\n",
      "Epoch 104, Loss: 0.211528941988945, Test Loss: 0.5896992683410645\n",
      "Epoch 105, Loss: 0.21289508044719696, Test Loss: 0.6306639313697815\n",
      "Epoch 106, Loss: 0.20898889005184174, Test Loss: 0.5984849333763123\n",
      "Epoch 107, Loss: 0.20757722854614258, Test Loss: 0.5877588987350464\n",
      "Epoch 108, Loss: 0.2049635499715805, Test Loss: 0.596737802028656\n",
      "Epoch 109, Loss: 0.2023889273405075, Test Loss: 0.59970623254776\n",
      "Epoch 110, Loss: 0.20302003622055054, Test Loss: 0.6001147627830505\n",
      "Epoch 111, Loss: 0.20068056881427765, Test Loss: 0.5974399447441101\n",
      "Epoch 112, Loss: 0.20193208754062653, Test Loss: 0.5948110222816467\n",
      "Epoch 113, Loss: 0.20013627409934998, Test Loss: 0.5802282691001892\n",
      "Epoch 114, Loss: 0.1987081915140152, Test Loss: 0.5979040265083313\n",
      "Epoch 115, Loss: 0.19808268547058105, Test Loss: 0.6107359528541565\n",
      "Epoch 116, Loss: 0.19939884543418884, Test Loss: 0.5976481437683105\n",
      "Epoch 117, Loss: 0.20043060183525085, Test Loss: 0.6357121467590332\n",
      "Epoch 118, Loss: 0.20279398560523987, Test Loss: 0.5932651162147522\n",
      "Epoch 119, Loss: 0.19466756284236908, Test Loss: 0.5803725123405457\n",
      "Epoch 120, Loss: 0.1927676498889923, Test Loss: 0.6094668507575989\n",
      "Epoch 121, Loss: 0.19389864802360535, Test Loss: 0.5859869718551636\n",
      "Epoch 122, Loss: 0.1934123933315277, Test Loss: 0.5891104340553284\n",
      "Epoch 123, Loss: 0.19189630448818207, Test Loss: 0.576917827129364\n",
      "Epoch 124, Loss: 0.1955239325761795, Test Loss: 0.6086879372596741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125, Loss: 0.19947224855422974, Test Loss: 0.5907110571861267\n",
      "Epoch 126, Loss: 0.19318105280399323, Test Loss: 0.5986579060554504\n",
      "Epoch 127, Loss: 0.18991929292678833, Test Loss: 0.5838176012039185\n",
      "Epoch 128, Loss: 0.19118422269821167, Test Loss: 0.5859792828559875\n",
      "Epoch 129, Loss: 0.18931294977664948, Test Loss: 0.596750795841217\n",
      "Epoch 130, Loss: 0.18977122008800507, Test Loss: 0.5858073234558105\n",
      "Epoch 131, Loss: 0.18919701874256134, Test Loss: 0.5547425150871277\n",
      "Epoch 132, Loss: 0.18832166492938995, Test Loss: 0.5946952700614929\n",
      "Epoch 133, Loss: 0.1893814206123352, Test Loss: 0.5840544104576111\n",
      "Epoch 134, Loss: 0.18814371526241302, Test Loss: 0.5688008666038513\n",
      "Epoch 135, Loss: 0.18588730692863464, Test Loss: 0.5661981701850891\n",
      "Epoch 136, Loss: 0.18505701422691345, Test Loss: 0.5869277119636536\n",
      "Epoch 137, Loss: 0.18408039212226868, Test Loss: 0.6085086464881897\n",
      "Epoch 138, Loss: 0.18570666015148163, Test Loss: 0.5572432279586792\n",
      "Epoch 139, Loss: 0.1855110228061676, Test Loss: 0.614395022392273\n",
      "Epoch 140, Loss: 0.18276382982730865, Test Loss: 0.5743803977966309\n",
      "Epoch 141, Loss: 0.18771548569202423, Test Loss: 0.6216083765029907\n",
      "Epoch 142, Loss: 0.19508479535579681, Test Loss: 0.5749285817146301\n",
      "Epoch 143, Loss: 0.18578675389289856, Test Loss: 0.5736724138259888\n",
      "Epoch 144, Loss: 0.18035681545734406, Test Loss: 0.5593072772026062\n",
      "Epoch 145, Loss: 0.1796424388885498, Test Loss: 0.5619199872016907\n",
      "Epoch 146, Loss: 0.17835165560245514, Test Loss: 0.5752719640731812\n",
      "Epoch 147, Loss: 0.17810778319835663, Test Loss: 0.5561370253562927\n",
      "Epoch 148, Loss: 0.1765042543411255, Test Loss: 0.5554835200309753\n",
      "Epoch 149, Loss: 0.17768000066280365, Test Loss: 0.5559208989143372\n",
      "Epoch 150, Loss: 0.17684899270534515, Test Loss: 0.5584372878074646\n",
      "Epoch 151, Loss: 0.1776721030473709, Test Loss: 0.5664463639259338\n",
      "Epoch 152, Loss: 0.17799630761146545, Test Loss: 0.5486186742782593\n",
      "Epoch 153, Loss: 0.17914777994155884, Test Loss: 0.5534024834632874\n",
      "Epoch 154, Loss: 0.1777694821357727, Test Loss: 0.55146723985672\n",
      "Epoch 155, Loss: 0.1778387427330017, Test Loss: 0.5739921927452087\n",
      "Epoch 156, Loss: 0.1780826896429062, Test Loss: 0.5874239802360535\n",
      "Epoch 157, Loss: 0.18017563223838806, Test Loss: 0.5767340660095215\n",
      "Epoch 158, Loss: 0.17553311586380005, Test Loss: 0.5458775162696838\n",
      "Epoch 159, Loss: 0.1755506843328476, Test Loss: 0.5435818433761597\n",
      "Epoch 160, Loss: 0.1715976893901825, Test Loss: 0.5505399107933044\n",
      "Epoch 161, Loss: 0.1698799431324005, Test Loss: 0.5533732175827026\n",
      "Epoch 162, Loss: 0.1701657474040985, Test Loss: 0.5485518574714661\n",
      "Epoch 163, Loss: 0.17093737423419952, Test Loss: 0.5661607980728149\n",
      "Epoch 164, Loss: 0.17273609340190887, Test Loss: 0.5704461336135864\n",
      "Epoch 165, Loss: 0.1719898134469986, Test Loss: 0.555711030960083\n",
      "Epoch 166, Loss: 0.17579424381256104, Test Loss: 0.5547744631767273\n",
      "Epoch 167, Loss: 0.17561256885528564, Test Loss: 0.5736717581748962\n",
      "Epoch 168, Loss: 0.1700134426355362, Test Loss: 0.550166666507721\n",
      "Epoch 169, Loss: 0.16980522871017456, Test Loss: 0.5354316234588623\n",
      "Epoch 170, Loss: 0.16741712391376495, Test Loss: 0.5386869311332703\n",
      "Epoch 171, Loss: 0.16855154931545258, Test Loss: 0.5466851592063904\n",
      "Epoch 172, Loss: 0.16691818833351135, Test Loss: 0.5388258099555969\n",
      "Epoch 173, Loss: 0.16711445152759552, Test Loss: 0.564289391040802\n",
      "Epoch 174, Loss: 0.1706254929304123, Test Loss: 0.5437940955162048\n",
      "Epoch 175, Loss: 0.17051522433757782, Test Loss: 0.5459814667701721\n",
      "Epoch 176, Loss: 0.16983772814273834, Test Loss: 0.5471732020378113\n",
      "Epoch 177, Loss: 0.1660623550415039, Test Loss: 0.5278419256210327\n",
      "Epoch 178, Loss: 0.16576255857944489, Test Loss: 0.5354660749435425\n",
      "Epoch 179, Loss: 0.16428539156913757, Test Loss: 0.5554150938987732\n",
      "Epoch 180, Loss: 0.16474498808383942, Test Loss: 0.5261249542236328\n",
      "Epoch 181, Loss: 0.16451789438724518, Test Loss: 0.53431236743927\n",
      "Epoch 182, Loss: 0.1657930612564087, Test Loss: 0.539064347743988\n",
      "Epoch 183, Loss: 0.17062236368656158, Test Loss: 0.5997442007064819\n",
      "Epoch 184, Loss: 0.17299793660640717, Test Loss: 0.5646854043006897\n",
      "Epoch 185, Loss: 0.1669195294380188, Test Loss: 0.5328294038772583\n",
      "Epoch 186, Loss: 0.16226746141910553, Test Loss: 0.5244625210762024\n",
      "Epoch 187, Loss: 0.16123133897781372, Test Loss: 0.5306400060653687\n",
      "Epoch 188, Loss: 0.16086071729660034, Test Loss: 0.5349870920181274\n",
      "Epoch 189, Loss: 0.15961404144763947, Test Loss: 0.5256790518760681\n",
      "Epoch 190, Loss: 0.16085368394851685, Test Loss: 0.5259611010551453\n",
      "Epoch 191, Loss: 0.16098898649215698, Test Loss: 0.5365831255912781\n",
      "Epoch 192, Loss: 0.16172705590724945, Test Loss: 0.5411093235015869\n",
      "Epoch 193, Loss: 0.16281385719776154, Test Loss: 0.5517179369926453\n",
      "Epoch 194, Loss: 0.16251613199710846, Test Loss: 0.5345705151557922\n",
      "Epoch 195, Loss: 0.16234466433525085, Test Loss: 0.5456632971763611\n",
      "Epoch 196, Loss: 0.1602671891450882, Test Loss: 0.5368381142616272\n",
      "Epoch 197, Loss: 0.1601102501153946, Test Loss: 0.5281301736831665\n",
      "Epoch 198, Loss: 0.15935485064983368, Test Loss: 0.5407720804214478\n",
      "Epoch 199, Loss: 0.1582237035036087, Test Loss: 0.5247530341148376\n",
      "Epoch 200, Loss: 0.15802912414073944, Test Loss: 0.5263110399246216\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "for epoch in range(EPOCHS):\n",
    "    for (data) in training_data:\n",
    "        train_step(auto_encoder,adam,data)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "\n",
    "    for (data) in test_data:\n",
    "        test_step(auto_encoder,data)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "  \n",
    "    template = 'Epoch {}, Loss: {}, Test Loss: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                         train_loss.result(), \n",
    "                         test_loss.result()\n",
    "                          ))\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "healthy_cases = data_new_features.loc[data_new_features['y']==0]\n",
    "analomies = data_new_features.loc[data_new_features['y']==1]\n",
    "healthy_cases.drop(columns = ['y'],inplace = True)\n",
    "analomies.drop(columns = ['y'],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_cases=scaler.transform(healthy_cases)\n",
    "analomies=scaler.transform(analomies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_cases=healthy_cases.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "analomies = analomies.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_cases = prepare_tensor(healthy_cases)\n",
    "analomies = prepare_tensor(analomies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analomies loss 3.0580782890319824\n"
     ]
    }
   ],
   "source": [
    "analomies_loss = tf.keras.metrics.MeanSquaredError()\n",
    "analomies = tf.data.Dataset.from_tensor_slices(analomies).batch(64,drop_remainder=True)\n",
    "for x in analomies:\n",
    "    prediction=auto_encoder(x)\n",
    "    loss= analomies_loss(x,prediction)\n",
    "print(\"analomies loss {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthy loss 0.2059439867734909\n"
     ]
    }
   ],
   "source": [
    "healthy_loss = tf.keras.metrics.MeanSquaredError()\n",
    "healthy_cases = tf.data.Dataset.from_tensor_slices(healthy_cases).batch(64,drop_remainder=True)\n",
    "for x in healthy_cases:\n",
    "    prediction=auto_encoder(x)\n",
    "    loss= healthy_loss(x,prediction)\n",
    "print(\"healthy loss {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
