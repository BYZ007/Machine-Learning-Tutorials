{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Universal Style Transfer.ipynb","provenance":[]}},"cells":[{"cell_type":"code","metadata":{"id":"4G4Q_iutH4wi","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","from tensorflow.keras import layers,Model,applications\n","import cv2\n","from matplotlib import pyplot as plt\n","%matplotlib inline\n","import os"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xOStDp0H4wk","colab_type":"code","colab":{}},"source":["from IPython.display import Latex\n","from IPython.display import Math"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AWih30FqH4wm","colab_type":"code","colab":{}},"source":["from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y5166pkGH4wo","colab_type":"text"},"source":["We start by building the WCT function which is described in https://papers.nips.cc/paper/6642-universal-style-transfer-via-feature-transforms.pdf:\n","We need a function that would create feature maps at each relevant layer we choose to compute the features for, and perform PCA via single value decomposition for the style and content feature maps in-order to match the covariance of the of the 2 feature maps. This is the key to fitting the style of the content image to that of the style image."]},{"cell_type":"code","metadata":{"id":"M-47msaOH4wo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"eb746206-5c88-4b3c-d3ae-aa59851ec590","executionInfo":{"status":"ok","timestamp":1570651485455,"user_tz":360,"elapsed":1909,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["print('Content feature map:')\n","Math(r'f_c  \\in R^{C\\times H_cW_c}')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Content feature map:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/latex":"$$f_c  \\in R^{C\\times H_cW_c}$$","text/plain":["<IPython.core.display.Math object>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"Txj2dT2GH4wr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"outputId":"8a7f2d4e-a2ef-42c6-942a-00f3834965fd","executionInfo":{"status":"ok","timestamp":1570651485456,"user_tz":360,"elapsed":815,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["print('Style feature map:')\n","Math(r'f_s  \\in R^{C\\times H_sW_s}')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Style feature map:\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/latex":"$$f_s  \\in R^{C\\times H_sW_s}$$","text/plain":["<IPython.core.display.Math object>"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"zFfVxlNNH4wt","colab_type":"text"},"source":["below are the expressions for the linear transforms for the feature maps"]},{"cell_type":"code","metadata":{"id":"sxwx_OKxH4wt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f088dab1-63b2-42bb-e22b-7895a46d710b","executionInfo":{"status":"ok","timestamp":1570651486708,"user_tz":360,"elapsed":340,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["%%latex\n","\\begin{align}\n","\\hat{f}_c = E_cD^{-\\frac{1}{2}}_cE^\\top_cf_c\\\\\n","\\hat{f}_{cs} = E_sD^{\\frac{1}{2}}_sE^\\top_s\\hat{f}_c\\\\\n","\\end{align}"],"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/latex":"\\begin{align}\n\\hat{f}_c = E_cD^{-\\frac{1}{2}}_cE^\\top_cf_c\\\\\n\\hat{f}_{cs} = E_sD^{\\frac{1}{2}}_sE^\\top_s\\hat{f}_c\\\\\n\\end{align}","text/plain":["<IPython.core.display.Latex object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"boaLowv5H4ww","colab_type":"text"},"source":["Ec is the corresponding orthogonal matrix of eigenvectors i.e. auto-covariance matrix, satisfying:"]},{"cell_type":"code","metadata":{"id":"36TcjW0nH4wx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"15bb7516-e3cd-4111-cd86-b3c871630bcc","executionInfo":{"status":"ok","timestamp":1570651488326,"user_tz":360,"elapsed":310,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["Math(r'f_c f^\\top_c = E_cD_cE^\\top_c')"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/latex":"$$f_c f^\\top_c = E_cD_cE^\\top_c$$","text/plain":["<IPython.core.display.Math object>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"ARYnZEfhH4w0","colab_type":"text"},"source":["The auto-covariance matrix can be factored using SVD into a matrix of eigen vectors,diagonal matrix of eigen values, and\n","the transpose of the eigen vectors.\n","tf.svd returns the eigen values in decending order or magnitude, we can use svd to perform PCA and remove eigen values which are too small. This will filter out noise within the feature space and performs the \"whitening\" where the low level features in the pixel space will be removed leaving only the higher level features, allowing feature space to be \"colored in\" later."]},{"cell_type":"code","metadata":{"id":"qQmQujM2H4w0","colab_type":"code","colab":{}},"source":["def WCT(content_features, style_features, alpha, eps = 1e-8):\n","    #assuming image = dim(1,W,H,C) convert to (C,W,H)\n","    content_transpose = tf.transpose(tf.squeeze(content_features),(2,0,1))\n","    style_transpose = tf.transpose(tf.squeeze(style_features),(2,0,1))\n","    \n","    #get the dimensions C,W,H\n","    Cc,Hc,Wc = tf.unstack(tf.shape(content_transpose))\n","    Cs,Hs,Ws = tf.unstack(tf.shape(style_transpose))\n","    \n","    # reshape to C,W*H\n","    content_feature_map = tf.reshape(content_transpose,(Cc,Hc*Wc))\n","    style_feature_map = tf.reshape(style_transpose,(Cs,Hs*Ws))\n","    \n","    #take the mean with respect to each channel and center the feature maps since we only care about\n","    # the second order statistics\n","    mc = tf.reduce_mean(content_feature_map,axis =1, keep_dims = True)\n","    ms = tf.reduce_mean(style_feature_map,axis =1, keep_dims = True)\n","    \n","    fc = content_feature_map-mc\n","    fs = style_feature_map-ms\n","    \n","    #auto-covariance matrices for style and content\n","    fcfcT = tf.matmul(fc,fc,transpose_b=True)/(tf.cast(Hc*Wc, tf.float32) - 1.) + tf.eye(Cc)*eps\n","    fsfsT = tf.matmul(fs,fs,transpose_b=True)/(tf.cast(Hs*Ws, tf.float32) - 1.) + tf.eye(Cs)*eps\n","    \n","    #find eigen values/vectors via SVD and perform PCA to filter features/channels that have small eigen values\n","    # this removes noise which \"whitens\" the feature space\n","    Dc,Ec,EcT = tf.linalg.svd(fcfcT)\n","    Ds,Es,EsT = tf.linalg.svd(fsfsT)\n","    \n","    k_c = tf.reduce_sum(tf.cast(tf.greater(Dc, 1e-5), tf.int32))\n","    k_s = tf.reduce_sum(tf.cast(tf.greater(Ds, 1e-5), tf.int32))    \n","    \n","    sqrt_Dc = tf.diag(tf.pow(Dc[:k_c],-0.5))\n","    sqrt_Ds = tf.diag(tf.pow(Ds[:k_s],0.5))\n","    \n","    #compute fcs_hat and recenter, computing fcs_hat is the inverse operation of the whitening and ensures that the \n","    #generated image has a feature map that matches the covariance of the style image\n","    fc_hat = (tf.matmul(tf.matmul(Ec[:,:k_c],sqrt_Dc),Ec[:,:k_c],transpose_b=True),fc)\n","    fcs_hat = (tf.matmul(tf.matmul(Es[:,:k_s],sqrt_Ds),Es[:,:k_s],transpose_b=True),fc_hat)\n","    fcs_hat += ms\n","    \n","    #create a blended feature map weighted by alpha and reshape it back into dims = (1,W,H,C) to be decoded\n","    blended = alpha*fcs_hat+(1-alpha)*(fc+mc)\n","    blended = tf.reshape(blended,(Cc,Hc,Wc))\n","    blended = tf.expand_dims(tf.transpose(blended,(1,2,0)),0)\n","    return blended"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUkLNnmdH4w2","colab_type":"code","colab":{}},"source":["class Encoder(Model):\n","    def __init__(self):\n","        super(Encoder,self).__init__()\n","        self.target_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\n","        self.vgg = VGG19(include_top = False,weights = 'imagenet')\n","        self.style_model = Model([self.vgg.input],[self.vgg.get_layer(name).output for name in self.target_layers])\n","    \n","    def call(self,inputs,target=None):\n","        outputs = self.style_model(inputs)\n","        if target!=None:\n","            return outputs[target]\n","        else:\n","            return outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Ri2PbV7H4w4","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"f1ae2932-ef2f-4be9-8df7-fe91b15bff3b","executionInfo":{"status":"ok","timestamp":1570651502100,"user_tz":360,"elapsed":2465,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["encoder= Encoder()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n","80142336/80134624 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7TPG2vUtH4w5","colab_type":"code","colab":{}},"source":["def Conv2DReflect(*args, **kwargs):\n","    return layers.Lambda(lambda x: layers.Conv2D(*args, **kwargs)(tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]],mode='REFLECT')))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cElcrbqbH4w7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":890},"outputId":"e5d47e8d-1d20-43e7-c4e1-5f277ed81c97","executionInfo":{"status":"ok","timestamp":1570651504909,"user_tz":360,"elapsed":301,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["encoder.vgg.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model: \"vgg19\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, None, None, 3)]   0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n","=================================================================\n","Total params: 20,024,384\n","Trainable params: 20,024,384\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"84HMIHGiH4w9","colab_type":"text"},"source":["Here we create 5 seperate decoders to perform the multilayered style transfer. Each decoder is identical to the inverse of the \n","VGG19 encoder layers for the target block."]},{"cell_type":"code","metadata":{"id":"bQQu_dp2H4w9","colab_type":"code","colab":{}},"source":["class Decoder5(Model):\n","    def __init__(self):\n","        super(Decoder5,self).__init__()\n","        self.block5_conv1 = Conv2DReflect(512, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block5_upsample = layers.UpSampling2D()\n","        self.block5_layers = [Conv2DReflect(512, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu') for i in range(3)]\n","        self.block4_conv1 = Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block4_upsample = layers.UpSampling2D()\n","        self.block4_layers = [Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu') for i in range(3)]\n","        self.block3_conv1 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block3_upsample = layers.UpSampling2D()\n","        self.block3_conv2 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_upsample = layers.UpSampling2D()\n","        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n","        \n","    def call(self,inputs):\n","        x = self.block5_conv1(inputs)\n","        x = self.block5_upsample(x)\n","        for i in range(3):\n","            x = self.block5_layers[i](x)\n","        x = self.block4_conv1(x)\n","        x = self.block4_upsample(x)\n","        for i in range(3):\n","            x = self.block4_layers[i](x)\n","        x = self.block3_conv1(x)\n","        x = self.block3_upsample(x)\n","        x = self.block3_conv2(x)\n","        x = self.block2_conv1(x)\n","        x = self.block2_upsample(x)\n","        x = self.block1_conv1(x)\n","        x = self.decoder_output(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSQaWjsRH4xA","colab_type":"code","colab":{}},"source":["class Decoder4(Model):\n","    def __init__(self):\n","        super(Decoder4,self).__init__()\n","        self.block4_conv1 = Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block4_upsample = layers.UpSampling2D()\n","        self.block4_layers = [Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu') for i in range(3)]\n","        self.block3_conv1 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block3_upsample = layers.UpSampling2D()\n","        self.block3_conv2 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_upsample = layers.UpSampling2D()\n","        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n","        \n","    def call(self,inputs):\n","        x = self.block4_conv1(x)\n","        x = self.block4_upsample(inputs)\n","        for i in range(3):\n","            x = self.block4_layers[i](x)\n","        x = self.block3_conv1(x)\n","        x = self.block3_upsample(x)\n","        x = self.block3_conv2(x)\n","        x = self.block2_conv1(x)\n","        x = self.block2_upsample(x)\n","        x = self.block1_conv1(x)\n","        x = self.decoder_output(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUW4em24H4xC","colab_type":"code","colab":{}},"source":["class Decoder3(Model):\n","    def __init__(self):\n","        super(Decoder3,self).__init__()\n","        self.block3_conv1 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block3_upsample = layers.UpSampling2D()\n","        self.block3_conv2 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_upsample = layers.UpSampling2D()\n","        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n","\n","    def call(self,inputs):\n","        x = self.block3_conv1(inputs)\n","        x = self.block3_upsample(x)\n","        x = self.block3_conv2(x)\n","        x = self.block2_conv1(x)\n","        x = self.block2_upsample(x)\n","        x = self.block1_conv1(x)\n","        x = self.decoder_output(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_v_GcUFH4xE","colab_type":"code","colab":{}},"source":["class Decoder2(Model):\n","    def __init__(self):\n","        super(Decoder2,self).__init__()\n","        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.block2_upsample = layers.UpSampling2D()\n","        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n","        \n","    def call(self,inputs):\n","        x = self.block2_conv1(inputs)\n","        x = self.block2_upsample(x)\n","        x = self.block1_conv1(x)\n","        x = self.decoder_output(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K5GBdzxGH4xF","colab_type":"code","colab":{}},"source":["class Decoder1(Model):\n","    def __init__(self):\n","        super(Decoder1,self).__init__()\n","        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n","        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n","\n","    def call(self,inputs):\n","        x = self.block1_conv1(inputs)\n","        x = self.decoder_output(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbzcE4fBH4xH","colab_type":"code","colab":{}},"source":["decoder1=Decoder1()\n","decoder2=Decoder2()\n","decoder3=Decoder3()\n","decoder4=Decoder4()\n","decoder5=Decoder5()\n","\n","decoders = [decoder1,decoder2,decoder3,decoder4,decoder5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OR4TcnGFH4xJ","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam(1e-4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GaXkGRQpH4xM","colab_type":"code","colab":{}},"source":["@tf.function\n","def compute_loss(input_features,training_image,decoded_image,decoded_features):\n","    reconstruction_loss = tf.reduce_mean(tf.keras.losses.MSE(training_image,decoded_image))\n","    feature_loss = tf.reduce_mean(tf.keras.losses.MSE(input_features,decoded_features))\n","    total_variation = tf.reduce_mean(tf.image.total_variation(decoded_image))\n","    \n","    total_loss = reconstruction_loss+feature_loss+0.01*total_variation\n","    return total_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TOVTbKiJH4xO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":404},"outputId":"f95818a6-3103-4078-bdd6-3849c5f3351b","executionInfo":{"status":"error","timestamp":1570652427437,"user_tz":360,"elapsed":904181,"user":{"displayName":"Boyang Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64","userId":"07054896143267778862"}}},"source":["import pathlib\n","data_dir = tf.keras.utils.get_file(origin='http://images.cocodataset.org/zips/train2014.zip',\n","                                         fname='coco_dataset', extract=True)\n","data_dir = pathlib.Path(data_dir)\n","\n","image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n","batchsize = 32\n","image_height = 224\n","image_width = 224"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Downloading data from http://images.cocodataset.org/zips/train2014.zip\n","13510574080/13510573713 [==============================] - 412s 0us/step\n"],"name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-86a09239e85e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m data_dir = tf.keras.utils.get_file(origin='http://images.cocodataset.org/zips/train2014.zip',\n\u001b[0;32m----> 3\u001b[0;31m                                          fname='coco_dataset', extract=True)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mextract\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m     \u001b[0m_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36m_extract_archive\u001b[0;34m(file_path, path, archive_format)\u001b[0m\n\u001b[1;32m    136\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mopen_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m           \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36mextractall\u001b[0;34m(self, path, members, pwd)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mzipinfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1524\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_member\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/zipfile.py\u001b[0m in \u001b[0;36m_extract_member\u001b[0;34m(self, member, targetpath, pwd)\u001b[0m\n\u001b[1;32m   1577\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmember\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpwd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m              \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargetpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1579\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargetpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/shutil.py\u001b[0m in \u001b[0;36mcopyfileobj\u001b[0;34m(fsrc, fdst, length)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mfdst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_samefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device"]}]},{"cell_type":"code","metadata":{"id":"4Piy9_mtH4xU","colab_type":"code","colab":{}},"source":["train_images = image_generator.flow_from_directory(directory=os.path.dirname(data_dir),batch_size=batchsize,\n","                                                     shuffle=True,target_size=(image_height, image_width))\n","@tf.function\n","def train_decoders(training_image,target_layer):\n","    with tf.GradientTape() as tape:\n","        decoded_image=decoders[target_layer-1](training_image)\n","        input_features = encoder(training_image,target=(target_layer-1))\n","        decoded_features = encoder(decoded_image,target = (target_layer-1))\n","        loss = compute_loss(input_features,training_image,decoded_image,decoded_features)\n","        \n","    graidents = tape.gradient(loss,decoders[target_layer-1].trainable_variables)\n","    optimizer.apply_gradients(zip(graidents,decoders[target_layer-1].trainable_variables))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lmO0Pr59H4xW","colab_type":"code","colab":{}},"source":["for target_layer in range(1,6):\n","    i=0\n","    for batch in train_images:\n","        i+=1\n","        if i==300:\n","            break\n","        elif i%100==0:\n","            print(\"decoder {} iteration {}\".format(target_layer,i))\n","        train_decoders(batch[0],target_layer)\n","    tf.saved_model.save(decoders[target_layer-1], r\"C:\\Users\\bzhang\\Documents\\Projects\\DecoderModels\\decoder{}\".format(target_layer))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-FV-XGjH4xX","colab_type":"code","colab":{}},"source":["def MultiStageStyleTransfer(encoder,decoders,content_image,style_image):\n","    for i in range(1,6):\n","        style_features = encoder(style_image,target = i-1)\n","        content_features = encoder(content_image,target=i-1)\n","        transformed_features = WCT(content_features,style_features,0.6)\n","        content_image = decoders[i-1]( transformed_features)\n","        content_image =tf.clip_by_value(content_image,clip_value_min = 0.0, clip_value_max = 1.0)\n","    return content_image"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7ljJ8U1LnDs","colab_type":"text"},"source":["# New Section"]}]}