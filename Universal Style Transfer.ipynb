{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4G4Q_iutH4wi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,applications\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2xOStDp0H4wk"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n",
    "from IPython.display import Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AWih30FqH4wm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y5166pkGH4wo"
   },
   "source": [
    "We start by building the WCT function which is described in https://papers.nips.cc/paper/6642-universal-style-transfer-via-feature-transforms.pdf:\n",
    "We need a function that would create feature maps at each relevant layer we choose to compute the features for, and perform PCA via single value decomposition for the style and content feature maps in-order to match the covariance of the of the 2 feature maps. This is the key to fitting the style of the content image to that of the style image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1909,
     "status": "ok",
     "timestamp": 1570651485455,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "M-47msaOH4wo",
    "outputId": "eb746206-5c88-4b3c-d3ae-aa59851ec590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content feature map:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle f_c  \\in R^{C\\times H_cW_c}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Content feature map:')\n",
    "Math(r'f_c  \\in R^{C\\times H_cW_c}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1570651485456,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "Txj2dT2GH4wr",
    "outputId": "8a7f2d4e-a2ef-42c6-942a-00f3834965fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style feature map:\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle f_s  \\in R^{C\\times H_sW_s}$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Style feature map:')\n",
    "Math(r'f_s  \\in R^{C\\times H_sW_s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zFfVxlNNH4wt"
   },
   "source": [
    "below are the expressions for the linear transforms for the feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1570651486708,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "sxwx_OKxH4wt",
    "outputId": "f088dab1-63b2-42bb-e22b-7895a46d710b"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "\\hat{f}_c = E_cD^{-\\frac{1}{2}}_cE^\\top_cf_c\\\\\n",
       "\\hat{f}_{cs} = E_sD^{\\frac{1}{2}}_sE^\\top_s\\hat{f}_c\\\\\n",
       "\\end{align}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "\\hat{f}_c = E_cD^{-\\frac{1}{2}}_cE^\\top_cf_c\\\\\n",
    "\\hat{f}_{cs} = E_sD^{\\frac{1}{2}}_sE^\\top_s\\hat{f}_c\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boaLowv5H4ww"
   },
   "source": [
    "Ec is the corresponding orthogonal matrix of eigenvectors i.e. auto-covariance matrix, satisfying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1570651488326,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "36TcjW0nH4wx",
    "outputId": "15bb7516-e3cd-4111-cd86-b3c871630bcc"
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle f_c f^\\top_c = E_cD_cE^\\top_c$"
      ],
      "text/plain": [
       "<IPython.core.display.Math object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Math(r'f_c f^\\top_c = E_cD_cE^\\top_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARYnZEfhH4w0"
   },
   "source": [
    "The auto-covariance matrix can be factored using SVD into a matrix of eigen vectors,diagonal matrix of eigen values, and\n",
    "the transpose of the eigen vectors.\n",
    "tf.svd returns the eigen values in decending order or magnitude, we can use svd to perform PCA and remove eigen values which are too small. This will filter out noise within the feature space and performs the \"whitening\" where the low level features in the pixel space will be removed leaving only the higher level features, allowing feature space to be \"colored in\" later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qQmQujM2H4w0"
   },
   "outputs": [],
   "source": [
    "def WCT(content_features, style_features, alpha, eps = 1e-8):\n",
    "    #assuming image = dim(1,W,H,C) convert to (C,W,H)\n",
    "    content_transpose = tf.transpose(tf.squeeze(content_features),(2,0,1))\n",
    "    style_transpose = tf.transpose(tf.squeeze(style_features),(2,0,1))\n",
    "    \n",
    "    #get the dimensions C,W,H\n",
    "    Cc,Hc,Wc = tf.unstack(tf.shape(content_transpose))\n",
    "    Cs,Hs,Ws = tf.unstack(tf.shape(style_transpose))\n",
    "    \n",
    "    # reshape to C,W*H\n",
    "    content_feature_map = tf.reshape(content_transpose,(Cc,Hc*Wc))\n",
    "    style_feature_map = tf.reshape(style_transpose,(Cs,Hs*Ws))\n",
    "    \n",
    "    #take the mean with respect to each channel and center the feature maps since we only care about\n",
    "    # the second order statistics\n",
    "    mc = tf.reduce_mean(content_feature_map,axis =1, keep_dims = True)\n",
    "    ms = tf.reduce_mean(style_feature_map,axis =1, keep_dims = True)\n",
    "    \n",
    "    fc = content_feature_map-mc\n",
    "    fs = style_feature_map-ms\n",
    "    \n",
    "    #auto-covariance matrices for style and content\n",
    "    fcfcT = tf.matmul(fc,fc,transpose_b=True)/(tf.cast(Hc*Wc, tf.float32) - 1.) + tf.eye(Cc)*eps\n",
    "    fsfsT = tf.matmul(fs,fs,transpose_b=True)/(tf.cast(Hs*Ws, tf.float32) - 1.) + tf.eye(Cs)*eps\n",
    "    \n",
    "    #find eigen values/vectors via SVD and perform PCA to filter features/channels that have small eigen values\n",
    "    # this removes noise which \"whitens\" the feature space\n",
    "    Dc,Ec,EcT = tf.linalg.svd(fcfcT)\n",
    "    Ds,Es,EsT = tf.linalg.svd(fsfsT)\n",
    "    \n",
    "    k_c = tf.reduce_sum(tf.cast(tf.greater(Dc, 1e-5), tf.int32))\n",
    "    k_s = tf.reduce_sum(tf.cast(tf.greater(Ds, 1e-5), tf.int32))    \n",
    "    \n",
    "    sqrt_Dc = tf.diag(tf.pow(Dc[:k_c],-0.5))\n",
    "    sqrt_Ds = tf.diag(tf.pow(Ds[:k_s],0.5))\n",
    "    \n",
    "    #compute fcs_hat and recenter, computing fcs_hat is the inverse operation of the whitening and ensures that the \n",
    "    #generated image has a feature map that matches the covariance of the style image\n",
    "    fc_hat = (tf.matmul(tf.matmul(Ec[:,:k_c],sqrt_Dc),Ec[:,:k_c],transpose_b=True),fc)\n",
    "    fcs_hat = (tf.matmul(tf.matmul(Es[:,:k_s],sqrt_Ds),Es[:,:k_s],transpose_b=True),fc_hat)\n",
    "    fcs_hat += ms\n",
    "    \n",
    "    #create a blended feature map weighted by alpha and reshape it back into dims = (1,W,H,C) to be decoded\n",
    "    blended = alpha*fcs_hat+(1-alpha)*(fc+mc)\n",
    "    blended = tf.reshape(blended,(Cc,Hc,Wc))\n",
    "    blended = tf.expand_dims(tf.transpose(blended,(1,2,0)),0)\n",
    "    return blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vUkLNnmdH4w2"
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.target_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\n",
    "        self.vgg = VGG19(include_top = False,weights = 'imagenet')\n",
    "        self.style_model = Model([self.vgg.input],[self.vgg.get_layer(name).output for name in self.target_layers])\n",
    "    \n",
    "    def call(self,inputs,target=None):\n",
    "        outputs = self.style_model(inputs)\n",
    "        if target!=None:\n",
    "            return outputs[target]\n",
    "        else:\n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2465,
     "status": "ok",
     "timestamp": 1570651502100,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "_Ri2PbV7H4w4",
    "outputId": "f1ae2932-ef2f-4be9-8df7-fe91b15bff3b"
   },
   "outputs": [],
   "source": [
    "encoder= Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7TPG2vUtH4w5"
   },
   "outputs": [],
   "source": [
    "def Conv2DReflect(*args, **kwargs):\n",
    "    return layers.Lambda(lambda x: layers.Conv2D(*args, **kwargs)(tf.pad(x, [[0, 0], [1, 1], [1, 1], [0, 0]],mode='REFLECT')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 301,
     "status": "ok",
     "timestamp": 1570651504909,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "cElcrbqbH4w7",
    "outputId": "e5d47e8d-1d20-43e7-c4e1-5f277ed81c97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder.vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84HMIHGiH4w9"
   },
   "source": [
    "Here we create 5 seperate decoders to perform the multilayered style transfer. Each decoder is identical to the inverse of the \n",
    "VGG19 encoder layers for the target block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQQu_dp2H4w9"
   },
   "outputs": [],
   "source": [
    "class Decoder5(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder5,self).__init__()\n",
    "        self.block5_conv1 = Conv2DReflect(512, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block5_upsample = layers.UpSampling2D()\n",
    "        self.block5_layers = [Conv2DReflect(512, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu') for i in range(3)]\n",
    "        self.block4_conv1 = Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block4_upsample = layers.UpSampling2D()\n",
    "        self.block4_layers = [Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu') for i in range(3)]\n",
    "        self.block3_conv1 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block3_upsample = layers.UpSampling2D()\n",
    "        self.block3_conv2 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_upsample = layers.UpSampling2D()\n",
    "        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x = self.block5_conv1(inputs)\n",
    "        x = self.block5_upsample(x)\n",
    "        for i in range(3):\n",
    "            x = self.block5_layers[i](x)\n",
    "        x = self.block4_conv1(x)\n",
    "        x = self.block4_upsample(x)\n",
    "        for i in range(3):\n",
    "            x = self.block4_layers[i](x)\n",
    "        x = self.block3_conv1(x)\n",
    "        x = self.block3_upsample(x)\n",
    "        x = self.block3_conv2(x)\n",
    "        x = self.block2_conv1(x)\n",
    "        x = self.block2_upsample(x)\n",
    "        x = self.block1_conv1(x)\n",
    "        x = self.decoder_output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZSQaWjsRH4xA"
   },
   "outputs": [],
   "source": [
    "class Decoder4(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder4,self).__init__()\n",
    "        self.block4_conv1 = Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block4_upsample = layers.UpSampling2D()\n",
    "        self.block4_layers = [Conv2DReflect(256, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu') for i in range(3)]\n",
    "        self.block3_conv1 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block3_upsample = layers.UpSampling2D()\n",
    "        self.block3_conv2 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_upsample = layers.UpSampling2D()\n",
    "        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x = self.block4_conv1(x)\n",
    "        x = self.block4_upsample(inputs)\n",
    "        for i in range(3):\n",
    "            x = self.block4_layers[i](x)\n",
    "        x = self.block3_conv1(x)\n",
    "        x = self.block3_upsample(x)\n",
    "        x = self.block3_conv2(x)\n",
    "        x = self.block2_conv1(x)\n",
    "        x = self.block2_upsample(x)\n",
    "        x = self.block1_conv1(x)\n",
    "        x = self.decoder_output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUW4em24H4xC"
   },
   "outputs": [],
   "source": [
    "class Decoder3(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder3,self).__init__()\n",
    "        self.block3_conv1 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block3_upsample = layers.UpSampling2D()\n",
    "        self.block3_conv2 = Conv2DReflect(128, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_upsample = layers.UpSampling2D()\n",
    "        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x = self.block3_conv1(inputs)\n",
    "        x = self.block3_upsample(x)\n",
    "        x = self.block3_conv2(x)\n",
    "        x = self.block2_conv1(x)\n",
    "        x = self.block2_upsample(x)\n",
    "        x = self.block1_conv1(x)\n",
    "        x = self.decoder_output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f_v_GcUFH4xE"
   },
   "outputs": [],
   "source": [
    "class Decoder2(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder2,self).__init__()\n",
    "        self.block2_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.block2_upsample = layers.UpSampling2D()\n",
    "        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x = self.block2_conv1(inputs)\n",
    "        x = self.block2_upsample(x)\n",
    "        x = self.block1_conv1(x)\n",
    "        x = self.decoder_output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5GBdzxGH4xF"
   },
   "outputs": [],
   "source": [
    "class Decoder1(Model):\n",
    "    def __init__(self):\n",
    "        super(Decoder1,self).__init__()\n",
    "        self.block1_conv1 = Conv2DReflect(64, kernel_size = (3,3), strides= 1, padding = 'valid',activation = 'relu')\n",
    "        self.decoder_output = Conv2DReflect(3,kernel_size = (3,3),strides = 1,padding='valid')\n",
    "\n",
    "    def call(self,inputs):\n",
    "        x = self.block1_conv1(inputs)\n",
    "        x = self.decoder_output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KbzcE4fBH4xH"
   },
   "outputs": [],
   "source": [
    "decoder1=Decoder1()\n",
    "decoder2=Decoder2()\n",
    "decoder3=Decoder3()\n",
    "decoder4=Decoder4()\n",
    "decoder5=Decoder5()\n",
    "\n",
    "decoders = [decoder1,decoder2,decoder3,decoder4,decoder5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OR4TcnGFH4xJ"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaXkGRQpH4xM"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(input_features,training_image,decoded_image,decoded_features):\n",
    "    reconstruction_loss = tf.reduce_mean(tf.keras.losses.MSE(training_image,decoded_image))\n",
    "    feature_loss = tf.reduce_mean(tf.keras.losses.MSE(input_features,decoded_features))\n",
    "    total_variation = tf.reduce_mean(tf.image.total_variation(decoded_image))\n",
    "    \n",
    "    total_loss = reconstruction_loss+feature_loss+0.01*total_variation\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 904181,
     "status": "error",
     "timestamp": 1570652427437,
     "user": {
      "displayName": "Boyang Zhang",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDhGNuuJ5Ct0Oe3Ni_YV0-QcD4ReyWwgbkzBm5g=s64",
      "userId": "07054896143267778862"
     },
     "user_tz": 360
    },
    "id": "TOVTbKiJH4xO",
    "outputId": "f95818a6-3103-4078-bdd6-3849c5f3351b"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "data_dir = tf.keras.utils.get_file(origin='http://images.cocodataset.org/zips/train2014.zip',\n",
    "                                         fname='coco_dataset', extract=True)\n",
    "data_dir = pathlib.Path(data_dir)\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "batchsize = 32\n",
    "image_height = 224\n",
    "image_width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Piy9_mtH4xU"
   },
   "outputs": [],
   "source": [
    "train_images = image_generator.flow_from_directory(directory=os.path.dirname(data_dir),batch_size=batchsize,\n",
    "                                                     shuffle=True,target_size=(image_height, image_width))\n",
    "@tf.function\n",
    "def train_decoders(training_image,target_layer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        decoded_image=decoders[target_layer-1](training_image)\n",
    "        input_features = encoder(training_image,target=(target_layer-1))\n",
    "        decoded_features = encoder(decoded_image,target = (target_layer-1))\n",
    "        loss = compute_loss(input_features,training_image,decoded_image,decoded_features)\n",
    "        \n",
    "    graidents = tape.gradient(loss,decoders[target_layer-1].trainable_variables)\n",
    "    optimizer.apply_gradients(zip(graidents,decoders[target_layer-1].trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lmO0Pr59H4xW"
   },
   "outputs": [],
   "source": [
    "for target_layer in range(1,6):\n",
    "    i=0\n",
    "    for batch in train_images:\n",
    "        i+=1\n",
    "        if i==300:\n",
    "            break\n",
    "        elif i%100==0:\n",
    "            print(\"decoder {} iteration {}\".format(target_layer,i))\n",
    "        train_decoders(batch[0],target_layer)\n",
    "    tf.saved_model.save(decoders[target_layer-1], r\"DecoderModels/decoder{}\".format(target_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V-FV-XGjH4xX"
   },
   "outputs": [],
   "source": [
    "def MultiStageStyleTransfer(encoder,decoders,content_image,style_image):\n",
    "    for i in range(1,6):\n",
    "        style_features = encoder(style_image,target = i-1)\n",
    "        content_features = encoder(content_image,target=i-1)\n",
    "        transformed_features = WCT(content_features,style_features,0.6)\n",
    "        content_image = decoders[i-1]( transformed_features)\n",
    "        content_image =tf.clip_by_value(content_image,clip_value_min = 0.0, clip_value_max = 1.0)\n",
    "    return content_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u7ljJ8U1LnDs"
   },
   "source": [
    "# New Section"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Universal Style Transfer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
