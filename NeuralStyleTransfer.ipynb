{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all required libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,Model,Sequential\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.applications.vgg19 import VGG19,preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the style and content images\n",
    "simg = cv2.imread('iris-scott-finger-painting-1.jpg')\n",
    "cimg = cv2.imread('paintacat.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load image helper function, it squares both images, convert to float 32, and add new bath dimension for conv2d, \n",
    "def load_img(img):\n",
    "    img = tf.image.resize(img,(600,600))\n",
    "    img = tf.cast(img,'float32')\n",
    "    img = img/255\n",
    "    img = img[tf.newaxis,:]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess reference images and define the trainable variable which is a (600,600,3) tensor of uniform random noise\n",
    "c_reference = load_img(cimg)\n",
    "s_reference = load_img(simg)\n",
    "gen_img = tf.random.uniform((1,600,600,3),minval=0.0,maxval=1.0,dtype='float32')\n",
    "gen_img = tf.Variable(gen_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gen_img = cv2.imread('StyleTransferExampleEpoch17.jpeg')\n",
    "#gen_img=gen_img/255\n",
    "#gen_img = gen_img[tf.newaxis,:]\n",
    "#gen_img = tf.Variable(gen_img,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#load in VGG19 from keras\n",
    "vgg = VGG19(include_top=False,weights = 'imagenet')\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the layer names of the content and style layers\n",
    "style_layers = ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\n",
    "content_layers = 'block5_conv2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model base on the VGG19 model, create a Model object that takes images as input and outputs the desired layers\n",
    "#the style and content models are model objects which are modified models of VGG19, the call function returns the outputs of these models\n",
    "#in a organized fashion\n",
    "\n",
    "class NeuralStyleTransfer(Model):\n",
    "    def __init__(self,style_layers,content_layers):\n",
    "        super(NeuralStyleTransfer,self).__init__()\n",
    "        self.vgg = VGG19(include_top=False,weights = 'imagenet')\n",
    "        self.vgg.trainable = False\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.style_model = Model([vgg.input],[vgg.get_layer(name).output for name in style_layers])\n",
    "        self.content_model = Model([vgg.input],[vgg.get_layer(content_layers).output])\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        inputs = preprocess_input(inputs*255)\n",
    "        style_out = self.style_model(inputs)\n",
    "        content_out = self.content_model(inputs)\n",
    "        return {'style':style_out,'content':content_out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the model\n",
    "model = NeuralStyleTransfer(style_layers,content_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the targets from the model\n",
    "style_target = model.style_model(preprocess_input(s_reference*255))\n",
    "content_target = model.content_model(preprocess_input(c_reference*255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion to compute the gram matrix, which is the inner product with respect to the dimensional elements of the filters, and then\n",
    "#compute the outer product of the filter vector with itself\n",
    "def gram_matrix(inputs):\n",
    "    input_shape = tf.shape(inputs)\n",
    "    G = tf.einsum('aijb,aijc->abc',inputs,inputs)\n",
    "    n = tf.cast((input_shape[1]*input_shape[2]),'float32')\n",
    "    return G/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the style and content losses\n",
    "def compute_loss(outputs,w_style = 1e-2,w_content = 1e4):\n",
    "    style_out = outputs['style']\n",
    "    content_out = outputs['content']\n",
    "    style_loss = 0\n",
    "    for i in range(len(style_out)):\n",
    "        style_loss += tf.reduce_mean((gram_matrix(style_target[i])-gram_matrix(style_out[i]))**2)\n",
    "    style_loss/=5\n",
    "    content_loss = tf.reduce_mean((content_target-content_out)**2)\n",
    "    return style_loss*w_style+content_loss*w_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the train step function which utilizes the tf.function for auto-graph and gradientTape for auto-differentiation\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01)\n",
    "@tf.function\n",
    "def train_step(gen_img):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(gen_img)\n",
    "        l = compute_loss(outputs)\n",
    "    grad = tape.gradient(l,gen_img)\n",
    "    optimizer.apply_gradients([(grad,gen_img)])\n",
    "    gen_img = tf.clip_by_value(gen_img,clip_value_min = 0.0, clip_value_max = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0619 15:06:05.310869 139665799026432 deprecation.py:323] From /home/zhang_boyang_00/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#train and save the intermediate images.\n",
    "epochs = 15\n",
    "steps_per_epoch = 100\n",
    "\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        train_step(gen_img)\n",
    "    img = np.array(tf.squeeze(gen_img))\n",
    "    cv2.imwrite('NeuralStyleTrainingSamples/StyleTransferExampleEpoch{}-4.jpeg'.format(n),img*255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
