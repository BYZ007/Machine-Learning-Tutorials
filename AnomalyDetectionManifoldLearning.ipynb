{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers,Model,utils\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,TensorBoard\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"rare_event_detection.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>...</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5/1/99 0:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376665</td>\n",
       "      <td>-4.596435</td>\n",
       "      <td>-4.095756</td>\n",
       "      <td>13.497687</td>\n",
       "      <td>-0.118830</td>\n",
       "      <td>-20.669883</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.091721</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>-4.936434</td>\n",
       "      <td>-24.590146</td>\n",
       "      <td>18.515436</td>\n",
       "      <td>3.473400</td>\n",
       "      <td>0.033444</td>\n",
       "      <td>0.953219</td>\n",
       "      <td>0.006076</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5/1/99 0:02</td>\n",
       "      <td>0</td>\n",
       "      <td>0.475720</td>\n",
       "      <td>-4.542502</td>\n",
       "      <td>-4.018359</td>\n",
       "      <td>16.230659</td>\n",
       "      <td>-0.128733</td>\n",
       "      <td>-18.758079</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.095871</td>\n",
       "      <td>0.062801</td>\n",
       "      <td>-4.937179</td>\n",
       "      <td>-32.413266</td>\n",
       "      <td>22.760065</td>\n",
       "      <td>2.682933</td>\n",
       "      <td>0.033536</td>\n",
       "      <td>1.090502</td>\n",
       "      <td>0.006083</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5/1/99 0:04</td>\n",
       "      <td>0</td>\n",
       "      <td>0.363848</td>\n",
       "      <td>-4.681394</td>\n",
       "      <td>-4.353147</td>\n",
       "      <td>14.127998</td>\n",
       "      <td>-0.138636</td>\n",
       "      <td>-17.836632</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.100265</td>\n",
       "      <td>0.072322</td>\n",
       "      <td>-4.937924</td>\n",
       "      <td>-34.183774</td>\n",
       "      <td>27.004663</td>\n",
       "      <td>3.537487</td>\n",
       "      <td>0.033629</td>\n",
       "      <td>1.840540</td>\n",
       "      <td>0.006090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5/1/99 0:06</td>\n",
       "      <td>0</td>\n",
       "      <td>0.301590</td>\n",
       "      <td>-4.758934</td>\n",
       "      <td>-4.023612</td>\n",
       "      <td>13.161567</td>\n",
       "      <td>-0.148142</td>\n",
       "      <td>-18.517601</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.104660</td>\n",
       "      <td>0.081600</td>\n",
       "      <td>-4.938669</td>\n",
       "      <td>-35.954281</td>\n",
       "      <td>21.672449</td>\n",
       "      <td>3.986095</td>\n",
       "      <td>0.033721</td>\n",
       "      <td>2.554880</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/1/99 0:08</td>\n",
       "      <td>0</td>\n",
       "      <td>0.265578</td>\n",
       "      <td>-4.749928</td>\n",
       "      <td>-4.333150</td>\n",
       "      <td>15.267340</td>\n",
       "      <td>-0.155314</td>\n",
       "      <td>-17.505913</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>...</td>\n",
       "      <td>10.109054</td>\n",
       "      <td>0.091121</td>\n",
       "      <td>-4.939414</td>\n",
       "      <td>-37.724789</td>\n",
       "      <td>21.907251</td>\n",
       "      <td>3.601573</td>\n",
       "      <td>0.033777</td>\n",
       "      <td>1.410494</td>\n",
       "      <td>0.006105</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          time  y        x1        x2        x3         x4        x5  \\\n",
       "0  5/1/99 0:00  0  0.376665 -4.596435 -4.095756  13.497687 -0.118830   \n",
       "1  5/1/99 0:02  0  0.475720 -4.542502 -4.018359  16.230659 -0.128733   \n",
       "2  5/1/99 0:04  0  0.363848 -4.681394 -4.353147  14.127998 -0.138636   \n",
       "3  5/1/99 0:06  0  0.301590 -4.758934 -4.023612  13.161567 -0.148142   \n",
       "4  5/1/99 0:08  0  0.265578 -4.749928 -4.333150  15.267340 -0.155314   \n",
       "\n",
       "          x6        x7        x8  ...        x52       x53       x54  \\\n",
       "0 -20.669883  0.000732 -0.061114  ...  10.091721  0.053279 -4.936434   \n",
       "1 -18.758079  0.000732 -0.061114  ...  10.095871  0.062801 -4.937179   \n",
       "2 -17.836632  0.010803 -0.061114  ...  10.100265  0.072322 -4.937924   \n",
       "3 -18.517601  0.002075 -0.061114  ...  10.104660  0.081600 -4.938669   \n",
       "4 -17.505913  0.000732 -0.061114  ...  10.109054  0.091121 -4.939414   \n",
       "\n",
       "         x55        x56       x57       x58       x59       x60  x61  \n",
       "0 -24.590146  18.515436  3.473400  0.033444  0.953219  0.006076    0  \n",
       "1 -32.413266  22.760065  2.682933  0.033536  1.090502  0.006083    0  \n",
       "2 -34.183774  27.004663  3.537487  0.033629  1.840540  0.006090    0  \n",
       "3 -35.954281  21.672449  3.986095  0.033721  2.554880  0.006097    0  \n",
       "4 -37.724789  21.907251  3.601573  0.033777  1.410494  0.006105    0  \n",
       "\n",
       "[5 rows x 63 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>...</th>\n",
       "      <th>x52</th>\n",
       "      <th>x53</th>\n",
       "      <th>x54</th>\n",
       "      <th>x55</th>\n",
       "      <th>x56</th>\n",
       "      <th>x57</th>\n",
       "      <th>x58</th>\n",
       "      <th>x59</th>\n",
       "      <th>x60</th>\n",
       "      <th>x61</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "      <td>18398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006740</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.157986</td>\n",
       "      <td>0.569300</td>\n",
       "      <td>-9.958345</td>\n",
       "      <td>0.006518</td>\n",
       "      <td>2.387533</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>-0.004125</td>\n",
       "      <td>-0.003056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.380519</td>\n",
       "      <td>0.360246</td>\n",
       "      <td>0.173708</td>\n",
       "      <td>2.379154</td>\n",
       "      <td>9.234953</td>\n",
       "      <td>0.233493</td>\n",
       "      <td>-0.001861</td>\n",
       "      <td>-0.061522</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.001033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.081822</td>\n",
       "      <td>0.742875</td>\n",
       "      <td>4.939762</td>\n",
       "      <td>5.937178</td>\n",
       "      <td>131.033712</td>\n",
       "      <td>0.634054</td>\n",
       "      <td>37.104012</td>\n",
       "      <td>0.108870</td>\n",
       "      <td>0.075460</td>\n",
       "      <td>0.156047</td>\n",
       "      <td>...</td>\n",
       "      <td>6.211598</td>\n",
       "      <td>14.174273</td>\n",
       "      <td>3.029516</td>\n",
       "      <td>67.940694</td>\n",
       "      <td>81.274103</td>\n",
       "      <td>2.326838</td>\n",
       "      <td>0.048732</td>\n",
       "      <td>10.394085</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.032120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.787279</td>\n",
       "      <td>-17.316550</td>\n",
       "      <td>-18.198509</td>\n",
       "      <td>-322.781610</td>\n",
       "      <td>-1.623988</td>\n",
       "      <td>-279.408440</td>\n",
       "      <td>-0.429273</td>\n",
       "      <td>-0.451141</td>\n",
       "      <td>-0.120087</td>\n",
       "      <td>...</td>\n",
       "      <td>-187.943440</td>\n",
       "      <td>-1817.595500</td>\n",
       "      <td>-8.210370</td>\n",
       "      <td>-230.574030</td>\n",
       "      <td>-269.039500</td>\n",
       "      <td>-12.640370</td>\n",
       "      <td>-0.149790</td>\n",
       "      <td>-100.810500</td>\n",
       "      <td>-0.012229</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.405681</td>\n",
       "      <td>-2.158235</td>\n",
       "      <td>-3.537054</td>\n",
       "      <td>-111.378372</td>\n",
       "      <td>-0.446787</td>\n",
       "      <td>-24.345268</td>\n",
       "      <td>-0.058520</td>\n",
       "      <td>-0.051043</td>\n",
       "      <td>-0.059966</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.672684</td>\n",
       "      <td>-1.928166</td>\n",
       "      <td>0.487780</td>\n",
       "      <td>-40.050046</td>\n",
       "      <td>-45.519149</td>\n",
       "      <td>-1.598804</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.295023</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.128245</td>\n",
       "      <td>-0.075505</td>\n",
       "      <td>-0.190683</td>\n",
       "      <td>-14.881585</td>\n",
       "      <td>-0.120745</td>\n",
       "      <td>10.528435</td>\n",
       "      <td>-0.009339</td>\n",
       "      <td>-0.000993</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294846</td>\n",
       "      <td>0.143612</td>\n",
       "      <td>0.702299</td>\n",
       "      <td>17.471317</td>\n",
       "      <td>1.438806</td>\n",
       "      <td>0.085826</td>\n",
       "      <td>0.012888</td>\n",
       "      <td>0.734591</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.421222</td>\n",
       "      <td>2.319297</td>\n",
       "      <td>3.421223</td>\n",
       "      <td>92.199134</td>\n",
       "      <td>0.325152</td>\n",
       "      <td>32.172974</td>\n",
       "      <td>0.060515</td>\n",
       "      <td>0.038986</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>...</td>\n",
       "      <td>5.109543</td>\n",
       "      <td>3.230770</td>\n",
       "      <td>2.675751</td>\n",
       "      <td>44.093387</td>\n",
       "      <td>63.209681</td>\n",
       "      <td>2.222118</td>\n",
       "      <td>0.020991</td>\n",
       "      <td>1.266506</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.054156</td>\n",
       "      <td>16.742105</td>\n",
       "      <td>15.900116</td>\n",
       "      <td>334.694098</td>\n",
       "      <td>4.239385</td>\n",
       "      <td>96.060768</td>\n",
       "      <td>1.705590</td>\n",
       "      <td>0.788826</td>\n",
       "      <td>4.060033</td>\n",
       "      <td>...</td>\n",
       "      <td>14.180588</td>\n",
       "      <td>11.148006</td>\n",
       "      <td>6.637265</td>\n",
       "      <td>287.252017</td>\n",
       "      <td>252.147455</td>\n",
       "      <td>6.922008</td>\n",
       "      <td>0.067249</td>\n",
       "      <td>6.985460</td>\n",
       "      <td>0.020510</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  y            x1            x2            x3            x4  \\\n",
       "count  18398.000000  18398.000000  18398.000000  18398.000000  18398.000000   \n",
       "mean       0.006740      0.011824      0.157986      0.569300     -9.958345   \n",
       "std        0.081822      0.742875      4.939762      5.937178    131.033712   \n",
       "min        0.000000     -3.787279    -17.316550    -18.198509   -322.781610   \n",
       "25%        0.000000     -0.405681     -2.158235     -3.537054   -111.378372   \n",
       "50%        0.000000      0.128245     -0.075505     -0.190683    -14.881585   \n",
       "75%        0.000000      0.421222      2.319297      3.421223     92.199134   \n",
       "max        1.000000      3.054156     16.742105     15.900116    334.694098   \n",
       "\n",
       "                 x5            x6            x7            x8            x9  \\\n",
       "count  18398.000000  18398.000000  18398.000000  18398.000000  18398.000000   \n",
       "mean       0.006518      2.387533      0.001647     -0.004125     -0.003056   \n",
       "std        0.634054     37.104012      0.108870      0.075460      0.156047   \n",
       "min       -1.623988   -279.408440     -0.429273     -0.451141     -0.120087   \n",
       "25%       -0.446787    -24.345268     -0.058520     -0.051043     -0.059966   \n",
       "50%       -0.120745     10.528435     -0.009339     -0.000993     -0.030057   \n",
       "75%        0.325152     32.172974      0.060515      0.038986      0.001990   \n",
       "max        4.239385     96.060768      1.705590      0.788826      4.060033   \n",
       "\n",
       "       ...           x52           x53           x54           x55  \\\n",
       "count  ...  18398.000000  18398.000000  18398.000000  18398.000000   \n",
       "mean   ...      0.380519      0.360246      0.173708      2.379154   \n",
       "std    ...      6.211598     14.174273      3.029516     67.940694   \n",
       "min    ...   -187.943440  -1817.595500     -8.210370   -230.574030   \n",
       "25%    ...     -3.672684     -1.928166      0.487780    -40.050046   \n",
       "50%    ...      0.294846      0.143612      0.702299     17.471317   \n",
       "75%    ...      5.109543      3.230770      2.675751     44.093387   \n",
       "max    ...     14.180588     11.148006      6.637265    287.252017   \n",
       "\n",
       "                x56           x57           x58           x59           x60  \\\n",
       "count  18398.000000  18398.000000  18398.000000  18398.000000  18398.000000   \n",
       "mean       9.234953      0.233493     -0.001861     -0.061522      0.001258   \n",
       "std       81.274103      2.326838      0.048732     10.394085      0.004721   \n",
       "min     -269.039500    -12.640370     -0.149790   -100.810500     -0.012229   \n",
       "25%      -45.519149     -1.598804      0.000470      0.295023     -0.001805   \n",
       "50%        1.438806      0.085826      0.012888      0.734591      0.000710   \n",
       "75%       63.209681      2.222118      0.020991      1.266506      0.004087   \n",
       "max      252.147455      6.922008      0.067249      6.985460      0.020510   \n",
       "\n",
       "                x61  \n",
       "count  18398.000000  \n",
       "mean       0.001033  \n",
       "std        0.032120  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%        0.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 62 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18274\n",
       "1      124\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that given 124 anomalies out of 18398 samples, the event that we are trying to detect is very rare ~0.674%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original paper, the features/columns of the dataset represents continous instrumentation data, likely analog force signals measured via loadcells, accelerometers, encoders, drive current loads etc. While x28 is categorical and x61 is binary. In the next few cells we will separate the continuous and categorical features in order to encode them more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ContinuousData = data.iloc[:,2:-1].drop(columns = [\"x28\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rates of change overtime of the features are likely to contribute more to the web breaks and failures that we are trying to detect. In order to capture that information we'll compute the rates of change via the first order difference and concatenate them as features to our existing dataset.\n",
    "\n",
    "A general rule of thumb is that for complex feature sets, where the features are not independent of eachother and redundancies may exist, the number of samples $N$ in the dataset must be larger than $\\sqrt{N}$ in order to avoid the \"curse of dimensionality\". The addition of first-order derivatives for the continuous features will take us to 120 features, and $120^2=14400 < 18274$. So we can cautiously conclude that we have enough data to avoid needing too many parameters in our model and risking overfitting. (we actually have 127 features after we one-hot encode the categorical data, but we're still ok)\n",
    "\n",
    "First order central difference is given by:\n",
    "\\begin{align}\n",
    "\\\\\n",
    "f'(x) \\approx \\frac{f(x+\\frac{1}{2}h)-f(x-\\frac{1}{2}h)}{h}\\\\\n",
    "\\end{align}\n",
    "\n",
    "Central difference is used since it has the most accurate approximation. The $h$ we will use is 4 mins.\n",
    "It is also convenient to ensure that the indexing of the data doesn't change regardless of the fact that the derivatives can't be computed for the first and last terms in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstOrderCentralDifference(data):\n",
    "    data_backward = np.array(data.copy().iloc[:-2,:])\n",
    "    data_forward = np.array(data.copy().iloc[2::,:])\n",
    "    \n",
    "    first_order_features = (data_forward-data_backward)/4.0\n",
    "    old_names = data.columns.values.tolist()\n",
    "    new_names = [('d'+name) for name in old_names]\n",
    "    first_order_features = pd.DataFrame(first_order_features,index = range(1,18397), columns = new_names)\n",
    "    \n",
    "    return pd.concat([data.drop([0,18397]),first_order_features],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataNewFeatures = FirstOrderCentralDifference(ContinuousData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>x9</th>\n",
       "      <th>x10</th>\n",
       "      <th>...</th>\n",
       "      <th>dx51</th>\n",
       "      <th>dx52</th>\n",
       "      <th>dx53</th>\n",
       "      <th>dx54</th>\n",
       "      <th>dx55</th>\n",
       "      <th>dx56</th>\n",
       "      <th>dx57</th>\n",
       "      <th>dx58</th>\n",
       "      <th>dx59</th>\n",
       "      <th>dx60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475720</td>\n",
       "      <td>-4.542502</td>\n",
       "      <td>-4.018359</td>\n",
       "      <td>16.230659</td>\n",
       "      <td>-0.128733</td>\n",
       "      <td>-18.758079</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>-0.059966</td>\n",
       "      <td>-0.038189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-2.398407</td>\n",
       "      <td>2.122307</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.221830</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.363848</td>\n",
       "      <td>-4.681394</td>\n",
       "      <td>-4.353147</td>\n",
       "      <td>14.127998</td>\n",
       "      <td>-0.138636</td>\n",
       "      <td>-17.836632</td>\n",
       "      <td>0.010803</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>-0.018352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-0.885254</td>\n",
       "      <td>-0.271904</td>\n",
       "      <td>0.325790</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.366094</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.301590</td>\n",
       "      <td>-4.758934</td>\n",
       "      <td>-4.023612</td>\n",
       "      <td>13.161567</td>\n",
       "      <td>-0.148142</td>\n",
       "      <td>-18.517601</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>-0.019986</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>-0.885254</td>\n",
       "      <td>-1.274353</td>\n",
       "      <td>0.016022</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>-0.107511</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.265578</td>\n",
       "      <td>-4.749928</td>\n",
       "      <td>-4.333150</td>\n",
       "      <td>15.267340</td>\n",
       "      <td>-0.155314</td>\n",
       "      <td>-17.505913</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-0.068024</td>\n",
       "      <td>0.410110</td>\n",
       "      <td>-0.080109</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>-0.417061</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.381253</td>\n",
       "      <td>-4.611746</td>\n",
       "      <td>-4.085072</td>\n",
       "      <td>14.143195</td>\n",
       "      <td>-0.162501</td>\n",
       "      <td>-16.494255</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>-0.061114</td>\n",
       "      <td>-0.030057</td>\n",
       "      <td>-0.008280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>-0.000372</td>\n",
       "      <td>1.374176</td>\n",
       "      <td>0.702812</td>\n",
       "      <td>0.128174</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.021938</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18392</th>\n",
       "      <td>-0.863607</td>\n",
       "      <td>0.735248</td>\n",
       "      <td>0.266091</td>\n",
       "      <td>133.555731</td>\n",
       "      <td>0.083242</td>\n",
       "      <td>26.922188</td>\n",
       "      <td>-0.139347</td>\n",
       "      <td>0.058823</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.038189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.331757</td>\n",
       "      <td>0.407589</td>\n",
       "      <td>-0.160218</td>\n",
       "      <td>-0.000384</td>\n",
       "      <td>0.136722</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18393</th>\n",
       "      <td>-0.877442</td>\n",
       "      <td>0.786430</td>\n",
       "      <td>0.406426</td>\n",
       "      <td>135.301215</td>\n",
       "      <td>0.112295</td>\n",
       "      <td>26.300392</td>\n",
       "      <td>-0.159185</td>\n",
       "      <td>0.058823</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.038189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>-3.560700</td>\n",
       "      <td>-0.987248</td>\n",
       "      <td>0.138733</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>-0.223778</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18394</th>\n",
       "      <td>-0.843988</td>\n",
       "      <td>0.633086</td>\n",
       "      <td>0.561918</td>\n",
       "      <td>133.228949</td>\n",
       "      <td>0.141332</td>\n",
       "      <td>25.678597</td>\n",
       "      <td>-0.159185</td>\n",
       "      <td>0.058823</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.038189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>-1.033875</td>\n",
       "      <td>-0.390385</td>\n",
       "      <td>-0.137344</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>-0.068666</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18395</th>\n",
       "      <td>-0.826547</td>\n",
       "      <td>0.450126</td>\n",
       "      <td>0.334582</td>\n",
       "      <td>134.977973</td>\n",
       "      <td>0.170370</td>\n",
       "      <td>25.056801</td>\n",
       "      <td>-0.159185</td>\n",
       "      <td>0.048752</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.038189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.414276</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013870</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.303889</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18396</th>\n",
       "      <td>-0.822842</td>\n",
       "      <td>0.419383</td>\n",
       "      <td>0.387263</td>\n",
       "      <td>135.658942</td>\n",
       "      <td>0.199422</td>\n",
       "      <td>24.435005</td>\n",
       "      <td>-0.159185</td>\n",
       "      <td>0.048752</td>\n",
       "      <td>-0.080108</td>\n",
       "      <td>-0.038189</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>-0.348114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.169388</td>\n",
       "      <td>-0.000217</td>\n",
       "      <td>-0.050507</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18396 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             x1        x2        x3          x4        x5         x6  \\\n",
       "1      0.475720 -4.542502 -4.018359   16.230659 -0.128733 -18.758079   \n",
       "2      0.363848 -4.681394 -4.353147   14.127998 -0.138636 -17.836632   \n",
       "3      0.301590 -4.758934 -4.023612   13.161567 -0.148142 -18.517601   \n",
       "4      0.265578 -4.749928 -4.333150   15.267340 -0.155314 -17.505913   \n",
       "5      0.381253 -4.611746 -4.085072   14.143195 -0.162501 -16.494255   \n",
       "...         ...       ...       ...         ...       ...        ...   \n",
       "18392 -0.863607  0.735248  0.266091  133.555731  0.083242  26.922188   \n",
       "18393 -0.877442  0.786430  0.406426  135.301215  0.112295  26.300392   \n",
       "18394 -0.843988  0.633086  0.561918  133.228949  0.141332  25.678597   \n",
       "18395 -0.826547  0.450126  0.334582  134.977973  0.170370  25.056801   \n",
       "18396 -0.822842  0.419383  0.387263  135.658942  0.199422  24.435005   \n",
       "\n",
       "             x7        x8        x9       x10  ...  dx51      dx52      dx53  \\\n",
       "1      0.000732 -0.061114 -0.059966 -0.038189  ...   0.0  0.002136  0.004761   \n",
       "2      0.010803 -0.061114 -0.030057 -0.018352  ...   0.0  0.002197  0.004700   \n",
       "3      0.002075 -0.061114 -0.019986 -0.008280  ...   0.0  0.002197  0.004700   \n",
       "4      0.000732 -0.061114 -0.030057 -0.008280  ...   0.0  0.002197  0.004761   \n",
       "5      0.000732 -0.061114 -0.030057 -0.008280  ...   0.0  0.002197  0.004700   \n",
       "...         ...       ...       ...       ...  ...   ...       ...       ...   \n",
       "18392 -0.139347  0.058823 -0.080108 -0.038189  ...   0.0  0.000000  0.000000   \n",
       "18393 -0.159185  0.058823 -0.080108 -0.038189  ...   0.0  0.000000  0.000000   \n",
       "18394 -0.159185  0.058823 -0.080108 -0.038189  ...   0.0  0.000000  0.000000   \n",
       "18395 -0.159185  0.048752 -0.080108 -0.038189  ...   0.0  0.000000  0.000000   \n",
       "18396 -0.159185  0.048752 -0.080108 -0.038189  ...   0.0  0.000000  0.000000   \n",
       "\n",
       "           dx54      dx55      dx56      dx57      dx58      dx59      dx60  \n",
       "1     -0.000373 -2.398407  2.122307  0.016022  0.000046  0.221830  0.000004  \n",
       "2     -0.000373 -0.885254 -0.271904  0.325790  0.000046  0.366094  0.000004  \n",
       "3     -0.000372 -0.885254 -1.274353  0.016022  0.000037 -0.107511  0.000004  \n",
       "4     -0.000373 -0.068024  0.410110 -0.080109 -0.000024 -0.417061  0.000004  \n",
       "5     -0.000372  1.374176  0.702812  0.128174 -0.000076 -0.021938  0.000004  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "18392  0.000462  0.331757  0.407589 -0.160218 -0.000384  0.136722  0.000004  \n",
       "18393  0.000463 -3.560700 -0.987248  0.138733  0.000200 -0.223778  0.000004  \n",
       "18394  0.000463 -1.033875 -0.390385 -0.137344  0.000776 -0.068666  0.000004  \n",
       "18395  0.000463  0.414276  0.000000  0.013870  0.000567  0.303889  0.000004  \n",
       "18396  0.000463 -0.348114  0.000000  0.169388 -0.000217 -0.050507  0.000004  \n",
       "\n",
       "[18396 rows x 118 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataNewFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we identify the indices of the anomalies (where y=1), and time shift it by 2 mins by shifting the indices back by 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([0,18397],inplace = True)\n",
    "y = data.y\n",
    "anomalies = y[y==1].index.copy()-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the standard scaler from sklearn to fit the continuous dataset without the anomalies, we will then use this scaler to transform the dataset before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(DataNewFeatures.drop(anomalies))\n",
    "DataNewFeatures.iloc[:,:] = scaler.transform(DataNewFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we take the categorical data x28 and apply one-hot encoding, and re-concatenate the categorical/binary data back onto the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca =PCA(n_components=40)\n",
    "pca.fit(DataNewFeatures.drop(anomalies))\n",
    "CompressedData = pd.DataFrame(pca.transform(DataNewFeatures),columns = ['pca_'+str(i) for i in range(1,41)],index = DataNewFeatures.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_1</th>\n",
       "      <th>pca_2</th>\n",
       "      <th>pca_3</th>\n",
       "      <th>pca_4</th>\n",
       "      <th>pca_5</th>\n",
       "      <th>pca_6</th>\n",
       "      <th>pca_7</th>\n",
       "      <th>pca_8</th>\n",
       "      <th>pca_9</th>\n",
       "      <th>pca_10</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_31</th>\n",
       "      <th>pca_32</th>\n",
       "      <th>pca_33</th>\n",
       "      <th>pca_34</th>\n",
       "      <th>pca_35</th>\n",
       "      <th>pca_36</th>\n",
       "      <th>pca_37</th>\n",
       "      <th>pca_38</th>\n",
       "      <th>pca_39</th>\n",
       "      <th>pca_40</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.167629</td>\n",
       "      <td>2.600991</td>\n",
       "      <td>-3.226725</td>\n",
       "      <td>1.756417</td>\n",
       "      <td>0.285213</td>\n",
       "      <td>-1.979013</td>\n",
       "      <td>-0.302688</td>\n",
       "      <td>-2.039714</td>\n",
       "      <td>1.109541</td>\n",
       "      <td>0.673106</td>\n",
       "      <td>...</td>\n",
       "      <td>1.477287</td>\n",
       "      <td>-0.821176</td>\n",
       "      <td>0.034368</td>\n",
       "      <td>0.195207</td>\n",
       "      <td>0.694806</td>\n",
       "      <td>-0.269063</td>\n",
       "      <td>-0.232485</td>\n",
       "      <td>0.328578</td>\n",
       "      <td>0.164787</td>\n",
       "      <td>0.281446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014363</td>\n",
       "      <td>2.742412</td>\n",
       "      <td>-3.392821</td>\n",
       "      <td>1.572611</td>\n",
       "      <td>-0.508399</td>\n",
       "      <td>-2.034450</td>\n",
       "      <td>0.481640</td>\n",
       "      <td>-1.747038</td>\n",
       "      <td>1.158701</td>\n",
       "      <td>0.418357</td>\n",
       "      <td>...</td>\n",
       "      <td>1.215598</td>\n",
       "      <td>-0.314943</td>\n",
       "      <td>-0.231679</td>\n",
       "      <td>0.818819</td>\n",
       "      <td>-0.982866</td>\n",
       "      <td>-0.600509</td>\n",
       "      <td>-0.228666</td>\n",
       "      <td>-0.025240</td>\n",
       "      <td>-0.212320</td>\n",
       "      <td>0.717907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.071939</td>\n",
       "      <td>2.845502</td>\n",
       "      <td>-3.590585</td>\n",
       "      <td>1.751235</td>\n",
       "      <td>-0.784058</td>\n",
       "      <td>-1.971026</td>\n",
       "      <td>0.558024</td>\n",
       "      <td>-1.770653</td>\n",
       "      <td>0.733993</td>\n",
       "      <td>-0.028914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.543751</td>\n",
       "      <td>0.379801</td>\n",
       "      <td>0.401707</td>\n",
       "      <td>0.813037</td>\n",
       "      <td>-0.948060</td>\n",
       "      <td>0.354322</td>\n",
       "      <td>-0.657913</td>\n",
       "      <td>0.238481</td>\n",
       "      <td>0.346375</td>\n",
       "      <td>0.189431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.004690</td>\n",
       "      <td>2.868572</td>\n",
       "      <td>-3.472332</td>\n",
       "      <td>1.838193</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>-1.859919</td>\n",
       "      <td>0.093462</td>\n",
       "      <td>-2.026112</td>\n",
       "      <td>0.843449</td>\n",
       "      <td>0.117172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165241</td>\n",
       "      <td>-0.130753</td>\n",
       "      <td>-2.057370</td>\n",
       "      <td>-1.463415</td>\n",
       "      <td>0.987264</td>\n",
       "      <td>-0.123735</td>\n",
       "      <td>1.175657</td>\n",
       "      <td>0.245622</td>\n",
       "      <td>0.829409</td>\n",
       "      <td>0.048033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.230351</td>\n",
       "      <td>2.727784</td>\n",
       "      <td>-3.105792</td>\n",
       "      <td>1.796413</td>\n",
       "      <td>0.057416</td>\n",
       "      <td>-2.177032</td>\n",
       "      <td>0.089564</td>\n",
       "      <td>-1.702631</td>\n",
       "      <td>0.805507</td>\n",
       "      <td>0.115606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524293</td>\n",
       "      <td>0.180993</td>\n",
       "      <td>-2.130938</td>\n",
       "      <td>-1.459710</td>\n",
       "      <td>0.381737</td>\n",
       "      <td>-0.418009</td>\n",
       "      <td>1.572110</td>\n",
       "      <td>-0.230047</td>\n",
       "      <td>0.288503</td>\n",
       "      <td>0.133784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18392</th>\n",
       "      <td>-0.136243</td>\n",
       "      <td>-1.177658</td>\n",
       "      <td>2.352381</td>\n",
       "      <td>-0.825732</td>\n",
       "      <td>0.306950</td>\n",
       "      <td>-0.572364</td>\n",
       "      <td>-1.183292</td>\n",
       "      <td>1.875247</td>\n",
       "      <td>0.489925</td>\n",
       "      <td>0.043598</td>\n",
       "      <td>...</td>\n",
       "      <td>1.183861</td>\n",
       "      <td>-0.467027</td>\n",
       "      <td>0.898818</td>\n",
       "      <td>1.146940</td>\n",
       "      <td>-0.916532</td>\n",
       "      <td>-0.777103</td>\n",
       "      <td>-0.126197</td>\n",
       "      <td>-0.722377</td>\n",
       "      <td>0.209026</td>\n",
       "      <td>-0.442740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18393</th>\n",
       "      <td>-0.054323</td>\n",
       "      <td>-1.098553</td>\n",
       "      <td>2.099750</td>\n",
       "      <td>-0.939712</td>\n",
       "      <td>-0.460810</td>\n",
       "      <td>-0.551409</td>\n",
       "      <td>-1.245552</td>\n",
       "      <td>1.716765</td>\n",
       "      <td>0.541223</td>\n",
       "      <td>0.006710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.856869</td>\n",
       "      <td>-0.139845</td>\n",
       "      <td>0.235947</td>\n",
       "      <td>1.248154</td>\n",
       "      <td>-2.447055</td>\n",
       "      <td>-0.197771</td>\n",
       "      <td>-1.184267</td>\n",
       "      <td>-0.077798</td>\n",
       "      <td>0.418340</td>\n",
       "      <td>-0.328965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18394</th>\n",
       "      <td>-0.171015</td>\n",
       "      <td>-1.021022</td>\n",
       "      <td>2.079271</td>\n",
       "      <td>-0.771249</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>-0.447343</td>\n",
       "      <td>-1.421327</td>\n",
       "      <td>1.678810</td>\n",
       "      <td>0.597291</td>\n",
       "      <td>0.154160</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.054152</td>\n",
       "      <td>0.612446</td>\n",
       "      <td>-1.080636</td>\n",
       "      <td>-1.055945</td>\n",
       "      <td>-0.968026</td>\n",
       "      <td>0.804839</td>\n",
       "      <td>0.547078</td>\n",
       "      <td>0.038820</td>\n",
       "      <td>0.449447</td>\n",
       "      <td>-0.283269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18395</th>\n",
       "      <td>-0.045101</td>\n",
       "      <td>-1.130354</td>\n",
       "      <td>2.366625</td>\n",
       "      <td>-0.623990</td>\n",
       "      <td>0.413168</td>\n",
       "      <td>-0.621332</td>\n",
       "      <td>-1.085385</td>\n",
       "      <td>1.972234</td>\n",
       "      <td>0.695140</td>\n",
       "      <td>-0.153862</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.124824</td>\n",
       "      <td>0.097913</td>\n",
       "      <td>-1.648953</td>\n",
       "      <td>-0.972487</td>\n",
       "      <td>-1.158619</td>\n",
       "      <td>0.401118</td>\n",
       "      <td>0.575117</td>\n",
       "      <td>-0.369345</td>\n",
       "      <td>0.771350</td>\n",
       "      <td>0.465586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18396</th>\n",
       "      <td>0.014857</td>\n",
       "      <td>-1.126658</td>\n",
       "      <td>2.168725</td>\n",
       "      <td>-0.664930</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>-0.622893</td>\n",
       "      <td>-0.953591</td>\n",
       "      <td>1.961174</td>\n",
       "      <td>0.534874</td>\n",
       "      <td>-0.185196</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478365</td>\n",
       "      <td>-0.511125</td>\n",
       "      <td>0.476110</td>\n",
       "      <td>0.278238</td>\n",
       "      <td>-0.958633</td>\n",
       "      <td>0.056466</td>\n",
       "      <td>-0.770995</td>\n",
       "      <td>0.093989</td>\n",
       "      <td>0.666284</td>\n",
       "      <td>-0.344845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18396 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          pca_1     pca_2     pca_3     pca_4     pca_5     pca_6     pca_7  \\\n",
       "1      0.167629  2.600991 -3.226725  1.756417  0.285213 -1.979013 -0.302688   \n",
       "2      0.014363  2.742412 -3.392821  1.572611 -0.508399 -2.034450  0.481640   \n",
       "3     -0.071939  2.845502 -3.590585  1.751235 -0.784058 -1.971026  0.558024   \n",
       "4      0.004690  2.868572 -3.472332  1.838193  0.003919 -1.859919  0.093462   \n",
       "5      0.230351  2.727784 -3.105792  1.796413  0.057416 -2.177032  0.089564   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "18392 -0.136243 -1.177658  2.352381 -0.825732  0.306950 -0.572364 -1.183292   \n",
       "18393 -0.054323 -1.098553  2.099750 -0.939712 -0.460810 -0.551409 -1.245552   \n",
       "18394 -0.171015 -1.021022  2.079271 -0.771249  0.012388 -0.447343 -1.421327   \n",
       "18395 -0.045101 -1.130354  2.366625 -0.623990  0.413168 -0.621332 -1.085385   \n",
       "18396  0.014857 -1.126658  2.168725 -0.664930  0.016430 -0.622893 -0.953591   \n",
       "\n",
       "          pca_8     pca_9    pca_10  ...    pca_31    pca_32    pca_33  \\\n",
       "1     -2.039714  1.109541  0.673106  ...  1.477287 -0.821176  0.034368   \n",
       "2     -1.747038  1.158701  0.418357  ...  1.215598 -0.314943 -0.231679   \n",
       "3     -1.770653  0.733993 -0.028914  ... -0.543751  0.379801  0.401707   \n",
       "4     -2.026112  0.843449  0.117172  ... -0.165241 -0.130753 -2.057370   \n",
       "5     -1.702631  0.805507  0.115606  ... -0.524293  0.180993 -2.130938   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "18392  1.875247  0.489925  0.043598  ...  1.183861 -0.467027  0.898818   \n",
       "18393  1.716765  0.541223  0.006710  ... -0.856869 -0.139845  0.235947   \n",
       "18394  1.678810  0.597291  0.154160  ... -1.054152  0.612446 -1.080636   \n",
       "18395  1.972234  0.695140 -0.153862  ... -1.124824  0.097913 -1.648953   \n",
       "18396  1.961174  0.534874 -0.185196  ... -0.478365 -0.511125  0.476110   \n",
       "\n",
       "         pca_34    pca_35    pca_36    pca_37    pca_38    pca_39    pca_40  \n",
       "1      0.195207  0.694806 -0.269063 -0.232485  0.328578  0.164787  0.281446  \n",
       "2      0.818819 -0.982866 -0.600509 -0.228666 -0.025240 -0.212320  0.717907  \n",
       "3      0.813037 -0.948060  0.354322 -0.657913  0.238481  0.346375  0.189431  \n",
       "4     -1.463415  0.987264 -0.123735  1.175657  0.245622  0.829409  0.048033  \n",
       "5     -1.459710  0.381737 -0.418009  1.572110 -0.230047  0.288503  0.133784  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "18392  1.146940 -0.916532 -0.777103 -0.126197 -0.722377  0.209026 -0.442740  \n",
       "18393  1.248154 -2.447055 -0.197771 -1.184267 -0.077798  0.418340 -0.328965  \n",
       "18394 -1.055945 -0.968026  0.804839  0.547078  0.038820  0.449447 -0.283269  \n",
       "18395 -0.972487 -1.158619  0.401118  0.575117 -0.369345  0.771350  0.465586  \n",
       "18396  0.278238 -0.958633  0.056466 -0.770995  0.093989  0.666284 -0.344845  \n",
       "\n",
       "[18396 rows x 40 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CompressedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x28 = pd.get_dummies(data.x28)\n",
    "x28 = x28.rename(columns = dict(zip(x28.columns.values.tolist(),[('x28_'+str(name)) for name in x28.columns.values.tolist()])))\n",
    "CompressedData = pd.concat([CompressedData,x28,data.x61],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we divide the dataframe into the training data and anomalous data, since we won't be using the anomalous data in the training set. Thus taking advantage of the fact that neural network models can't extrapolate very effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = CompressedData.drop(anomalies)\n",
    "anomalous_data = CompressedData.iloc[anomalies-1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the data we need to ensure that all sequences that would include the anomalies are removed, then we divide the dataset into sequences and return it as a np array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreprocessData(data,seq_len=15):\n",
    "    dataset = np.array(data,'float32')\n",
    "    s = set(range(0,len(data)-seq_len))\n",
    "    for i in anomalies:\n",
    "        s.difference_update(set(range(i-seq_len+1,i+1)))\n",
    "    sequences = []\n",
    "    for i in list(s):\n",
    "        sequences.append(dataset[i:i+seq_len])\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the dataset will ultimately be (samples,sequence length, feature length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16581, 15, 49)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = PreprocessData(training_data)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: (100, 15, 49), types: tf.float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dataset = tf.data.Dataset.from_tensor_slices(dataset).shuffle(1000).batch(100,drop_remainder=True)\n",
    "batched_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3 # kernel size\n",
    "latent_dim = 256 # dimension of the latent z\n",
    "n_filters=128 # initial filter depth for encoder\n",
    "depth = 3 # depth of dialated TCN\n",
    "seq_len = 15 # length of the time sequence\n",
    "feature_dim = 49 # feature dimension for input tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    def __init__(self,k,latent_dim,n_filters,depth):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.depth = depth\n",
    "        \n",
    "        self.tcn_block =[\n",
    "                        layers.Conv1D(n_filters*2**(i),kernel_size = k,strides = 1, \n",
    "                        padding = 'causal',dilation_rate = 2**i,activation = 'elu')\n",
    "                        for i in range(depth)\n",
    "                        ]\n",
    "        \n",
    "        self.dropouts = [layers.Dropout(0.3) for i in range(depth)]\n",
    "        self.flat = layers.Flatten()\n",
    "        self.dense = layers.Dense(latent_dim*2)\n",
    "        \n",
    "        \n",
    "    def call(self,inputs,training = True):\n",
    "        # input dim = (100,15,49)\n",
    "        x = inputs\n",
    "        for i in range(self.depth):\n",
    "            x = self.tcn_block[i](x)\n",
    "            x = self.dropouts[i](x,training = training) \n",
    "        # dim = (100,15,512)\n",
    "        x = self.flat(x)\n",
    "        x = self.dense(x)\n",
    "        # output dim = (100,512)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    def __init__(self,k,latent_dim,n_filters,depth,seq_len,feature_dim):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.depth = depth\n",
    "        self.dense1 = layers.Dense(seq_len*n_filters*2**(depth-1),activation = 'elu')\n",
    "        self.reshape = layers.Reshape((seq_len,n_filters*2**(depth-1)))\n",
    "        self.conv_layers =  [\n",
    "                            layers.Conv1D(n_filters*2**(depth-1-i),\n",
    "                            kernel_size = k, strides=1, padding = 'SAME', activation = 'elu')\n",
    "                            for i in range(depth)\n",
    "                            ]\n",
    "        self.dropouts = [layers.Dropout(0.3) for i in range(depth)]\n",
    "        self.generated_sequence = layers.Conv1D(feature_dim, kernel_size = k, strides = 1, padding = 'SAME')\n",
    "\n",
    "    def call(self,x,training = True):\n",
    "        # input dim = (100,256)\n",
    "        x = self.dense1(x)\n",
    "        # dim = (100,15*256)\n",
    "        x = self.reshape(x)\n",
    "        # dim = (100,15,256)\n",
    "        for i in range(self.depth):\n",
    "            x = self.conv_layers[i](x)\n",
    "            x = self.dropouts[i](x,training = training)\n",
    "        # dim = (100,15,128)\n",
    "        x = self.generated_sequence(x)\n",
    "        # output dim = (100,15,49)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reparameterize(mean,logvar,noise):\n",
    "    z=noise*tf.exp(logvar * 0.5)+mean\n",
    "    return z\n",
    "\n",
    "def logPDF(mean,logvar,z):\n",
    "    logf = -0.5*((z-mean)**2/(tf.math.exp(logvar)+1e-8)+tf.math.log(2*np.pi))\n",
    "    return logf\n",
    "\n",
    "def LossFunction(z,mean,logvar,generated_sequence,input_sequence,beta=0.3):\n",
    "    logpz = tf.reduce_sum(logPDF(0.0,0.0,z),axis = -1)\n",
    "    logqz = tf.reduce_sum(logPDF(mean,logvar,z),axis = -1)\n",
    "    logpxz = tf.reduce_sum(tf.keras.losses.MSE(input_sequence,generated_sequence))\n",
    "    \n",
    "    total_loss = beta*tf.reduce_mean(-logpz+logqz-logpxz)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BVAE(Model):\n",
    "    def __init__(self,k,latent_dim,n_filters,depth,seq_len,feature_dim):\n",
    "        super(BVAE,self).__init__()\n",
    "        self.encoder = Encoder(k,latent_dim,n_filters,depth)\n",
    "        self.decoder = Decoder(k,latent_dim,n_filters,depth,seq_len,feature_dim)\n",
    "        self.latent_dim = latent_dim\n",
    "    \n",
    "    def call(self,input_sequence,training = True):\n",
    "        x = self.encoder(input_sequence)\n",
    "        mean = x[:,:self.latent_dim]\n",
    "        logvar = x[:,self.latent_dim::]\n",
    "        noise = tf.random.normal(shape = (100,self.latent_dim))\n",
    "        z = Reparameterize(mean,logvar,noise)\n",
    "        generated_sequence = self.decoder(z)\n",
    "        return z,mean,logvar,generated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "model = BVAE(k,latent_dim,n_filters,depth,seq_len,feature_dim)\n",
    "train_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "\n",
    "@tf.function\n",
    "def train_step(input_sequence):\n",
    "    with tf.GradientTape() as tape:\n",
    "        z,mean,logvar,generated_sequence = model(input_sequence)\n",
    "        loss = LossFunction(z,mean,logvar,generated_sequence,input_sequence)\n",
    "        \n",
    "    gradients = tape.gradient(loss,model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss epoch 1: nan\n",
      "training loss epoch 2: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-da60bd519f6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m201\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minput_sequence\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatched_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrain_summary_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Tensorflow2\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in range(1,201):\n",
    "    for input_sequence in (batched_dataset):\n",
    "        train_step(input_sequence)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "    print(\"training loss epoch {}: {}\".format(epoch,train_loss.result()))\n",
    "    train_loss.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9264), started 0:03:00 ago. (Use '!kill 9264' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d1a3763e863569b1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d1a3763e863569b1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
